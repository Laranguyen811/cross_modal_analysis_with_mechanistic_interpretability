{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c611aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc13051a80d43faa56520096807a226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_config.json:   0%|          | 0.00/471 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official model name open-clip:laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759d1c3fc24d41c6b2943e78d898694e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual projection shape torch.Size([1024, 768])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading weights from the architecture is not currently supported: None, generated from model name custom. Feel free to open an issue on GitHub to request this feature.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:523\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    522\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(axes_lengths), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhashable_axes_lengths\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:234\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct_from_shape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# shape or one of passed axes lengths is not hashable (i.e. they are symbols)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:183\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape, axes_dims)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(length, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(known_product, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m length \u001b[38;5;241m!=\u001b[39m known_product:\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mknown_product\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# assert len(unknown_axes) == 1, 'this is enforced when recipe is created, so commented out'\u001b[39;00m\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, 1024 != 1020",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/ViT-Prisma/src/vit_prisma/prisma_tools/loading_from_pretrained.py:528\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, is_timm, is_clip, cfg, hf_model, dtype, return_old_state_dict, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m     old_state_dict \u001b[38;5;241m=\u001b[39m load_state_dict(checkpoint_path)\n\u001b[0;32m--> 528\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_open_clip_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_clip:\n",
      "File \u001b[0;32m/workspace/ViT-Prisma/src/vit_prisma/prisma_tools/loading_from_pretrained.py:86\u001b[0m, in \u001b[0;36mconvert_open_clip_weights\u001b[0;34m(old_state_dict, cfg, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Reshape Q, K, V weights\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m W_Q \u001b[38;5;241m=\u001b[39m \u001b[43meinops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(h dh) d -> h d dh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m W_K \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(W_K, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(h dh) d -> h d dh\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_heads, d\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39md_model, dh\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39md_head)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03meinops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03mThis operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    532\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"(h dh) d -> h d dh\".\n Input tensor shape: torch.Size([1024, 1024]). Additional info: {'h': 12, 'd': 1024, 'dh': 85}.\n Shape mismatch, 1024 != 1020",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# og_model_name = 'laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K'\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# og_model_name = 'laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K'\u001b[39;00m\n\u001b[1;32m     16\u001b[0m og_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m hooked_model \u001b[38;5;241m=\u001b[39m \u001b[43mHookedViT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen-clip:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mog_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_timm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# in future, do all models\u001b[39;00m\n\u001b[1;32m     20\u001b[0m hooked_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m hooked_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/workspace/ViT-Prisma/src/vit_prisma/models/base_vit.py:774\u001b[0m, in \u001b[0;36mHookedViT.from_pretrained\u001b[0;34m(cls, model_name, is_timm, is_clip, fold_ln, center_writing_weights, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, use_attn_result, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# Set up other parts of transformer\u001b[39;00m\n\u001b[1;32m    768\u001b[0m cfg \u001b[38;5;241m=\u001b[39m convert_pretrained_model_config(\n\u001b[1;32m    769\u001b[0m     model_name,\n\u001b[1;32m    770\u001b[0m     is_timm\u001b[38;5;241m=\u001b[39mis_timm,\n\u001b[1;32m    771\u001b[0m     is_clip\u001b[38;5;241m=\u001b[39mis_clip,\n\u001b[1;32m    772\u001b[0m )\n\u001b[0;32m--> 774\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_pretrained_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_timm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_old_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(cfg, move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# set false if openclip; not working properly\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/ViT-Prisma/src/vit_prisma/prisma_tools/loading_from_pretrained.py:565\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, is_timm, is_clip, cfg, hf_model, dtype, return_old_state_dict, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_dict\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights from the architecture is not currently supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39moriginal_architecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, generated from model name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Feel free to open an issue on GitHub to request this feature.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Loading weights from the architecture is not currently supported: None, generated from model name custom. Feel free to open an issue on GitHub to request this feature."
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import timm\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "import open_clip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from vit_prisma.model_eval.evaluate_imagenet import zero_shot_eval\n",
    "from vit_prisma.dataloaders.imagenet_dataset import load_imagenet\n",
    "\n",
    "# og_model_name = 'laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K'\n",
    "# og_model_name = 'laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K'\n",
    "og_model_name = 'laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K'\n",
    "# find models that seem to currently work, and ones that dont'\n",
    "\n",
    "hooked_model = HookedViT.from_pretrained('open-clip:' + og_model_name, is_timm=False, is_clip=True, fold_ln=False, center_writing_weights=False) # in future, do all models\n",
    "hooked_model.to(\"cuda\")\n",
    "hooked_model.eval()\n",
    "hooked_state_dict = hooked_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hf-hub:' + og_model_name\n",
    "og_model, *data = open_clip.create_model_and_transforms(model_name)\n",
    "og_model.eval()\n",
    "# all_outputs, layer_names = get_all_layer_outputs(og_model, random_input)\n",
    "og_state_dict = og_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd188ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_embedding\n",
      "text_projection\n",
      "logit_scale\n",
      "visual.class_embedding\n",
      "visual.positional_embedding\n",
      "visual.proj\n",
      "visual.conv1.weight\n",
      "visual.ln_pre.weight\n",
      "visual.ln_pre.bias\n",
      "visual.transformer.resblocks.0.ln_1.weight\n",
      "visual.transformer.resblocks.0.ln_1.bias\n",
      "visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "visual.transformer.resblocks.0.ln_2.weight\n",
      "visual.transformer.resblocks.0.ln_2.bias\n",
      "visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.1.ln_1.weight\n",
      "visual.transformer.resblocks.1.ln_1.bias\n",
      "visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "visual.transformer.resblocks.1.ln_2.weight\n",
      "visual.transformer.resblocks.1.ln_2.bias\n",
      "visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.2.ln_1.weight\n",
      "visual.transformer.resblocks.2.ln_1.bias\n",
      "visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "visual.transformer.resblocks.2.ln_2.weight\n",
      "visual.transformer.resblocks.2.ln_2.bias\n",
      "visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.3.ln_1.weight\n",
      "visual.transformer.resblocks.3.ln_1.bias\n",
      "visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "visual.transformer.resblocks.3.ln_2.weight\n",
      "visual.transformer.resblocks.3.ln_2.bias\n",
      "visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.4.ln_1.weight\n",
      "visual.transformer.resblocks.4.ln_1.bias\n",
      "visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "visual.transformer.resblocks.4.ln_2.weight\n",
      "visual.transformer.resblocks.4.ln_2.bias\n",
      "visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.5.ln_1.weight\n",
      "visual.transformer.resblocks.5.ln_1.bias\n",
      "visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "visual.transformer.resblocks.5.ln_2.weight\n",
      "visual.transformer.resblocks.5.ln_2.bias\n",
      "visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.6.ln_1.weight\n",
      "visual.transformer.resblocks.6.ln_1.bias\n",
      "visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "visual.transformer.resblocks.6.ln_2.weight\n",
      "visual.transformer.resblocks.6.ln_2.bias\n",
      "visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.7.ln_1.weight\n",
      "visual.transformer.resblocks.7.ln_1.bias\n",
      "visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "visual.transformer.resblocks.7.ln_2.weight\n",
      "visual.transformer.resblocks.7.ln_2.bias\n",
      "visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.8.ln_1.weight\n",
      "visual.transformer.resblocks.8.ln_1.bias\n",
      "visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "visual.transformer.resblocks.8.ln_2.weight\n",
      "visual.transformer.resblocks.8.ln_2.bias\n",
      "visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.9.ln_1.weight\n",
      "visual.transformer.resblocks.9.ln_1.bias\n",
      "visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "visual.transformer.resblocks.9.ln_2.weight\n",
      "visual.transformer.resblocks.9.ln_2.bias\n",
      "visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.10.ln_1.weight\n",
      "visual.transformer.resblocks.10.ln_1.bias\n",
      "visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "visual.transformer.resblocks.10.ln_2.weight\n",
      "visual.transformer.resblocks.10.ln_2.bias\n",
      "visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.11.ln_1.weight\n",
      "visual.transformer.resblocks.11.ln_1.bias\n",
      "visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "visual.transformer.resblocks.11.ln_2.weight\n",
      "visual.transformer.resblocks.11.ln_2.bias\n",
      "visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "visual.ln_post.weight\n",
      "visual.ln_post.bias\n",
      "transformer.resblocks.0.ln_1.weight\n",
      "transformer.resblocks.0.ln_1.bias\n",
      "transformer.resblocks.0.attn.in_proj_weight\n",
      "transformer.resblocks.0.attn.in_proj_bias\n",
      "transformer.resblocks.0.attn.out_proj.weight\n",
      "transformer.resblocks.0.attn.out_proj.bias\n",
      "transformer.resblocks.0.ln_2.weight\n",
      "transformer.resblocks.0.ln_2.bias\n",
      "transformer.resblocks.0.mlp.c_fc.weight\n",
      "transformer.resblocks.0.mlp.c_fc.bias\n",
      "transformer.resblocks.0.mlp.c_proj.weight\n",
      "transformer.resblocks.0.mlp.c_proj.bias\n",
      "transformer.resblocks.1.ln_1.weight\n",
      "transformer.resblocks.1.ln_1.bias\n",
      "transformer.resblocks.1.attn.in_proj_weight\n",
      "transformer.resblocks.1.attn.in_proj_bias\n",
      "transformer.resblocks.1.attn.out_proj.weight\n",
      "transformer.resblocks.1.attn.out_proj.bias\n",
      "transformer.resblocks.1.ln_2.weight\n",
      "transformer.resblocks.1.ln_2.bias\n",
      "transformer.resblocks.1.mlp.c_fc.weight\n",
      "transformer.resblocks.1.mlp.c_fc.bias\n",
      "transformer.resblocks.1.mlp.c_proj.weight\n",
      "transformer.resblocks.1.mlp.c_proj.bias\n",
      "transformer.resblocks.2.ln_1.weight\n",
      "transformer.resblocks.2.ln_1.bias\n",
      "transformer.resblocks.2.attn.in_proj_weight\n",
      "transformer.resblocks.2.attn.in_proj_bias\n",
      "transformer.resblocks.2.attn.out_proj.weight\n",
      "transformer.resblocks.2.attn.out_proj.bias\n",
      "transformer.resblocks.2.ln_2.weight\n",
      "transformer.resblocks.2.ln_2.bias\n",
      "transformer.resblocks.2.mlp.c_fc.weight\n",
      "transformer.resblocks.2.mlp.c_fc.bias\n",
      "transformer.resblocks.2.mlp.c_proj.weight\n",
      "transformer.resblocks.2.mlp.c_proj.bias\n",
      "transformer.resblocks.3.ln_1.weight\n",
      "transformer.resblocks.3.ln_1.bias\n",
      "transformer.resblocks.3.attn.in_proj_weight\n",
      "transformer.resblocks.3.attn.in_proj_bias\n",
      "transformer.resblocks.3.attn.out_proj.weight\n",
      "transformer.resblocks.3.attn.out_proj.bias\n",
      "transformer.resblocks.3.ln_2.weight\n",
      "transformer.resblocks.3.ln_2.bias\n",
      "transformer.resblocks.3.mlp.c_fc.weight\n",
      "transformer.resblocks.3.mlp.c_fc.bias\n",
      "transformer.resblocks.3.mlp.c_proj.weight\n",
      "transformer.resblocks.3.mlp.c_proj.bias\n",
      "transformer.resblocks.4.ln_1.weight\n",
      "transformer.resblocks.4.ln_1.bias\n",
      "transformer.resblocks.4.attn.in_proj_weight\n",
      "transformer.resblocks.4.attn.in_proj_bias\n",
      "transformer.resblocks.4.attn.out_proj.weight\n",
      "transformer.resblocks.4.attn.out_proj.bias\n",
      "transformer.resblocks.4.ln_2.weight\n",
      "transformer.resblocks.4.ln_2.bias\n",
      "transformer.resblocks.4.mlp.c_fc.weight\n",
      "transformer.resblocks.4.mlp.c_fc.bias\n",
      "transformer.resblocks.4.mlp.c_proj.weight\n",
      "transformer.resblocks.4.mlp.c_proj.bias\n",
      "transformer.resblocks.5.ln_1.weight\n",
      "transformer.resblocks.5.ln_1.bias\n",
      "transformer.resblocks.5.attn.in_proj_weight\n",
      "transformer.resblocks.5.attn.in_proj_bias\n",
      "transformer.resblocks.5.attn.out_proj.weight\n",
      "transformer.resblocks.5.attn.out_proj.bias\n",
      "transformer.resblocks.5.ln_2.weight\n",
      "transformer.resblocks.5.ln_2.bias\n",
      "transformer.resblocks.5.mlp.c_fc.weight\n",
      "transformer.resblocks.5.mlp.c_fc.bias\n",
      "transformer.resblocks.5.mlp.c_proj.weight\n",
      "transformer.resblocks.5.mlp.c_proj.bias\n",
      "transformer.resblocks.6.ln_1.weight\n",
      "transformer.resblocks.6.ln_1.bias\n",
      "transformer.resblocks.6.attn.in_proj_weight\n",
      "transformer.resblocks.6.attn.in_proj_bias\n",
      "transformer.resblocks.6.attn.out_proj.weight\n",
      "transformer.resblocks.6.attn.out_proj.bias\n",
      "transformer.resblocks.6.ln_2.weight\n",
      "transformer.resblocks.6.ln_2.bias\n",
      "transformer.resblocks.6.mlp.c_fc.weight\n",
      "transformer.resblocks.6.mlp.c_fc.bias\n",
      "transformer.resblocks.6.mlp.c_proj.weight\n",
      "transformer.resblocks.6.mlp.c_proj.bias\n",
      "transformer.resblocks.7.ln_1.weight\n",
      "transformer.resblocks.7.ln_1.bias\n",
      "transformer.resblocks.7.attn.in_proj_weight\n",
      "transformer.resblocks.7.attn.in_proj_bias\n",
      "transformer.resblocks.7.attn.out_proj.weight\n",
      "transformer.resblocks.7.attn.out_proj.bias\n",
      "transformer.resblocks.7.ln_2.weight\n",
      "transformer.resblocks.7.ln_2.bias\n",
      "transformer.resblocks.7.mlp.c_fc.weight\n",
      "transformer.resblocks.7.mlp.c_fc.bias\n",
      "transformer.resblocks.7.mlp.c_proj.weight\n",
      "transformer.resblocks.7.mlp.c_proj.bias\n",
      "transformer.resblocks.8.ln_1.weight\n",
      "transformer.resblocks.8.ln_1.bias\n",
      "transformer.resblocks.8.attn.in_proj_weight\n",
      "transformer.resblocks.8.attn.in_proj_bias\n",
      "transformer.resblocks.8.attn.out_proj.weight\n",
      "transformer.resblocks.8.attn.out_proj.bias\n",
      "transformer.resblocks.8.ln_2.weight\n",
      "transformer.resblocks.8.ln_2.bias\n",
      "transformer.resblocks.8.mlp.c_fc.weight\n",
      "transformer.resblocks.8.mlp.c_fc.bias\n",
      "transformer.resblocks.8.mlp.c_proj.weight\n",
      "transformer.resblocks.8.mlp.c_proj.bias\n",
      "transformer.resblocks.9.ln_1.weight\n",
      "transformer.resblocks.9.ln_1.bias\n",
      "transformer.resblocks.9.attn.in_proj_weight\n",
      "transformer.resblocks.9.attn.in_proj_bias\n",
      "transformer.resblocks.9.attn.out_proj.weight\n",
      "transformer.resblocks.9.attn.out_proj.bias\n",
      "transformer.resblocks.9.ln_2.weight\n",
      "transformer.resblocks.9.ln_2.bias\n",
      "transformer.resblocks.9.mlp.c_fc.weight\n",
      "transformer.resblocks.9.mlp.c_fc.bias\n",
      "transformer.resblocks.9.mlp.c_proj.weight\n",
      "transformer.resblocks.9.mlp.c_proj.bias\n",
      "transformer.resblocks.10.ln_1.weight\n",
      "transformer.resblocks.10.ln_1.bias\n",
      "transformer.resblocks.10.attn.in_proj_weight\n",
      "transformer.resblocks.10.attn.in_proj_bias\n",
      "transformer.resblocks.10.attn.out_proj.weight\n",
      "transformer.resblocks.10.attn.out_proj.bias\n",
      "transformer.resblocks.10.ln_2.weight\n",
      "transformer.resblocks.10.ln_2.bias\n",
      "transformer.resblocks.10.mlp.c_fc.weight\n",
      "transformer.resblocks.10.mlp.c_fc.bias\n",
      "transformer.resblocks.10.mlp.c_proj.weight\n",
      "transformer.resblocks.10.mlp.c_proj.bias\n",
      "transformer.resblocks.11.ln_1.weight\n",
      "transformer.resblocks.11.ln_1.bias\n",
      "transformer.resblocks.11.attn.in_proj_weight\n",
      "transformer.resblocks.11.attn.in_proj_bias\n",
      "transformer.resblocks.11.attn.out_proj.weight\n",
      "transformer.resblocks.11.attn.out_proj.bias\n",
      "transformer.resblocks.11.ln_2.weight\n",
      "transformer.resblocks.11.ln_2.bias\n",
      "transformer.resblocks.11.mlp.c_fc.weight\n",
      "transformer.resblocks.11.mlp.c_fc.bias\n",
      "transformer.resblocks.11.mlp.c_proj.weight\n",
      "transformer.resblocks.11.mlp.c_proj.bias\n",
      "token_embedding.weight\n",
      "ln_final.weight\n",
      "ln_final.bias\n"
     ]
    }
   ],
   "source": [
    "for k, v in og_state_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502db74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOLERANCE = 1e-5\n",
    "for i in range(len())\n",
    "og_tensor = og_state_dict[\"visual.transformer.resblocks.0.ln_1.bias\"]\n",
    "hooked_tensor = hooked_state_dict['blocks.0.ln1.b']\n",
    "assert torch.allclose(hooked_tensor.cpu(), og_tensor, atol=TOLERANCE), f\"Model output diverges! Max diff: {torch.max(torch.abs(hooked_output - output))}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150fc3cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.4004e-04,  1.5547e-02,  1.1443e-04, -2.5516e-04,  3.7344e-02,\n",
       "         6.8169e-02, -2.2151e-01,  1.2305e-01, -4.7701e-05,  4.3652e-02,\n",
       "        -1.3883e-01,  2.2224e-02,  3.1400e-03, -2.6446e-01, -1.9172e-04,\n",
       "         7.2726e-02,  9.1304e-02, -6.9150e-02,  6.0655e-04, -6.4259e-02,\n",
       "        -2.6761e-01, -6.3850e-02, -1.3749e-01, -2.0366e-02, -1.1364e-01,\n",
       "        -4.0331e-02,  7.0269e-02, -4.3323e-02,  3.5753e-01, -2.8951e-02,\n",
       "        -9.9992e-02, -8.4994e-04, -2.4468e-02, -5.2385e-02, -3.8524e-02,\n",
       "         6.3792e-04, -4.4348e-03,  1.7679e-01,  1.4868e-01,  3.4615e-01,\n",
       "         1.6235e-01, -7.9912e-02,  1.0218e-01,  4.7762e-02,  1.1078e-05,\n",
       "         1.8023e-03,  9.5139e-02, -5.1023e-02,  4.8207e-03,  1.8138e-02,\n",
       "         2.5796e-03,  7.8956e-02, -1.2881e-01, -2.5910e-02, -5.2114e-03,\n",
       "         8.1150e-02, -1.9824e-02,  2.1170e-05,  4.8710e-03, -2.7062e-01,\n",
       "         1.3506e-02,  3.9497e-02,  1.2379e-02,  2.1648e-02, -2.7365e-02,\n",
       "         2.0319e-01, -6.3227e-02, -5.1607e-02,  1.7295e-02,  1.3664e-01,\n",
       "        -6.6649e-02, -2.2080e-04,  1.5750e-02, -2.3084e-02,  1.0495e-03,\n",
       "        -9.8622e-02, -2.1366e-02,  3.4277e-05, -2.5390e-02, -1.1292e-01,\n",
       "        -9.8050e-02, -1.8265e-04,  6.1661e-02,  8.7809e-03, -5.6602e-02,\n",
       "         1.1661e-04,  2.2367e-02,  1.1256e-01,  1.2226e-01,  2.0021e-03,\n",
       "        -4.5702e-03,  1.6678e-01,  7.1395e-02,  5.1393e-04, -4.3480e-02,\n",
       "        -4.1367e-02,  4.4709e-04,  5.4882e-02, -1.3758e-01, -1.4060e-04,\n",
       "         4.2128e-01,  6.4923e-04,  4.0555e-04, -9.8343e-02, -1.0764e-02,\n",
       "        -2.5830e-01,  1.4377e-01,  1.9013e-01, -1.5254e-02, -8.8506e-03,\n",
       "         1.1477e-02, -3.3598e-01, -1.6540e-04, -5.5699e-04,  4.9909e-02,\n",
       "         9.0955e-02, -8.3932e-05,  2.8427e-03,  2.3548e-02, -2.2977e-01,\n",
       "         6.2530e-03, -2.9120e-01,  2.4793e-02,  1.1321e-01, -1.7752e-01,\n",
       "         8.2360e-04, -1.2158e-03,  7.5567e-02,  1.5903e-01, -2.0287e-01,\n",
       "         1.7382e-02,  2.7978e-02,  1.8872e-01, -3.4206e-02,  2.5710e-02,\n",
       "        -3.1813e-01, -2.8135e-02, -1.7887e-02,  1.3268e-03, -7.1234e-02,\n",
       "        -3.9153e-01, -9.1991e-04,  2.2538e-01,  7.9559e-02,  8.6730e-02,\n",
       "         2.8904e-01, -2.8560e-01, -7.3606e-02, -1.5836e-02,  2.7400e-01,\n",
       "        -1.4882e-02, -6.8840e-03,  5.0094e-03, -1.4853e-01,  1.9250e-01,\n",
       "         8.4492e-02,  1.5955e-02, -1.5349e-01, -2.5036e-01, -1.3879e-01,\n",
       "        -5.7912e-04,  4.6501e-02, -1.7446e-02, -8.1824e-02,  4.8804e-04,\n",
       "        -1.7781e-01, -8.0029e-02,  8.0600e-02,  4.8528e-03, -2.0018e-02,\n",
       "         5.3650e-02,  6.6022e-02,  9.9316e-02,  1.3975e-01,  5.4099e-02,\n",
       "         5.9351e-02, -1.4506e-02,  5.7632e-04,  3.4466e-01,  1.2584e-01,\n",
       "         2.8570e-02, -1.3919e-01, -3.0227e-02,  2.1479e-01,  6.1539e-02,\n",
       "         4.1191e-01, -1.9534e-01, -1.1296e-01, -8.4605e-02,  3.2953e-02,\n",
       "        -4.4872e-02, -2.8180e-01,  1.8373e-01, -2.4052e-02,  8.5291e-02,\n",
       "         2.8671e-02,  6.8167e-02, -4.6095e-02, -1.7146e-03, -2.5384e-04,\n",
       "         3.0720e-01,  1.7533e-01, -2.5039e-03, -8.9430e-02,  1.6281e-03,\n",
       "        -9.2651e-02,  3.2987e-02, -1.7221e-05,  8.2233e-02,  3.0135e-02,\n",
       "         7.9699e-02,  2.7639e-02, -7.4402e-02, -1.4628e-01,  6.8674e-02,\n",
       "        -1.3812e-02, -1.4699e-01,  2.5234e-02, -1.9108e-01,  1.2735e-01,\n",
       "        -6.3571e-02, -3.2842e-01,  1.5928e-01,  1.0166e-01, -9.5491e-02,\n",
       "         1.8244e-02, -1.0157e-01,  1.5943e-01,  4.1023e-01,  1.4648e-03,\n",
       "         1.1488e-01,  2.0162e-01,  9.1368e-02,  1.9622e-04, -2.9316e-02,\n",
       "        -4.0671e-02,  1.0903e-01,  1.0954e-01, -1.5449e-01, -4.7697e-02,\n",
       "        -2.7791e-02, -7.9599e-03, -8.0061e-02, -3.6353e-03,  2.4927e-04,\n",
       "        -1.1275e-03, -7.7331e-02, -9.5923e-03,  5.8110e-02, -9.9101e-02,\n",
       "        -7.7977e-02,  3.7906e-01, -6.7343e-02,  8.8664e-02, -5.8153e-02,\n",
       "         7.3247e-03, -8.8239e-02,  2.1293e-01,  9.9175e-02,  1.2003e-02,\n",
       "         2.1518e-01,  7.9398e-02,  3.6135e-02, -5.0748e-03,  6.2041e-02,\n",
       "        -2.7146e-02,  5.7961e-04,  5.7335e-02, -1.8677e-02, -5.0348e-02,\n",
       "         4.3455e-01,  1.5110e-02, -1.0475e-01,  3.5692e-02,  1.9759e-01,\n",
       "         1.7842e-03, -4.1909e-05, -2.9352e-02, -2.8309e-01, -2.3182e-03,\n",
       "         1.5341e-02,  6.6213e-02, -7.4137e-02, -2.5314e-04,  9.8726e-04,\n",
       "         8.5236e-02, -6.7166e-02,  5.9791e-02, -9.1404e-02, -2.4568e-02,\n",
       "        -1.9090e-03, -9.7865e-04,  4.7338e-02,  1.0620e-01,  1.4780e-01,\n",
       "         7.6617e-04,  3.0129e-02, -9.8124e-04,  2.2512e-01, -5.2414e-04,\n",
       "         6.0470e-04,  8.4479e-02,  9.2111e-02,  5.0325e-04, -1.6954e-01,\n",
       "        -9.9732e-02,  1.5064e-01,  3.2428e-02,  6.5680e-02,  4.1545e-02,\n",
       "        -6.6156e-05, -9.0956e-04,  1.0056e-01, -3.5699e-02,  2.7285e-02,\n",
       "         6.4173e-02,  3.3491e-01, -9.7724e-02,  1.2449e-01, -9.8282e-04,\n",
       "         7.3098e-02, -6.4294e-02,  5.5671e-02,  2.5822e-02, -4.0224e-04,\n",
       "         9.6178e-01, -5.7480e-02,  3.7623e-01, -8.9061e-02,  1.2464e-01,\n",
       "         8.8725e-04, -1.1090e-01,  1.2189e-04, -2.1884e-01,  3.6156e-02,\n",
       "        -3.1405e-04,  2.2374e-03, -1.1618e-03, -1.3030e-01,  8.7413e-03,\n",
       "        -1.6229e-01, -1.9270e-02,  3.1752e-02, -1.0918e-01,  7.0276e-02,\n",
       "         6.1208e-02, -9.7448e-02, -1.1022e-01,  2.8551e-03, -7.4570e-02,\n",
       "         7.3743e-02, -1.0949e-01,  4.9151e-02, -1.1366e-03, -1.0124e-02,\n",
       "        -7.8827e-02,  9.0588e-04,  7.0769e-03,  6.7701e-02,  3.7486e-04,\n",
       "        -5.5932e-02, -9.9676e-02,  6.3592e-02,  1.8093e-01, -4.8160e-02,\n",
       "         1.0736e-01,  1.5293e-02, -3.0901e-01, -8.3350e-06,  4.0838e-04,\n",
       "        -2.1467e-01,  2.6399e-02, -2.6324e-04, -1.6416e-01,  2.2701e-02,\n",
       "        -2.6170e-04, -2.4388e-04,  4.4249e-02, -3.7792e-02, -4.0168e-02,\n",
       "         3.1859e-02, -1.8747e-01, -7.0880e-03,  2.3951e-02, -1.0346e-02,\n",
       "        -6.3756e-02, -1.0866e-01,  1.4680e-01,  4.8045e-02, -8.4171e-02,\n",
       "        -1.3456e-01, -1.1332e-01,  5.9682e-01,  3.4385e-02, -6.0843e-02,\n",
       "        -1.1704e-01,  2.4422e-02,  1.8723e-02,  1.7452e-01,  8.3422e-02,\n",
       "        -2.0591e-01, -2.1573e-02,  5.3128e-02, -7.7725e-02, -1.5428e-02,\n",
       "        -1.2198e-02,  1.2746e-01,  7.9414e-04, -1.8920e-02, -5.3817e-02,\n",
       "        -4.4739e-01,  7.2493e-02, -3.1372e-02, -1.2261e-01,  1.6727e-02,\n",
       "         4.7413e-02, -1.2631e-03, -9.4757e-02,  3.5834e-02, -3.3941e-02,\n",
       "        -3.4799e-02,  5.3317e-02,  1.6142e-01, -4.3102e-02, -9.3018e-02,\n",
       "         2.1222e-02,  4.8347e-02,  9.1428e-02,  2.6013e-01, -3.6606e-04,\n",
       "         2.1686e-04, -6.6778e-02, -2.4917e-02,  4.9077e-01,  7.0667e-02,\n",
       "        -2.2207e-03,  1.1292e-01,  1.5435e-01,  1.6694e-01, -7.2930e-02,\n",
       "         3.3292e-04, -1.4773e-01,  2.3937e-01,  4.8174e-04,  6.0665e-02,\n",
       "         8.8716e-02, -3.4184e-02,  1.4542e-01, -6.9053e-02, -5.7849e-02,\n",
       "        -9.2947e-02, -9.2739e-02,  1.8066e-01,  1.3084e-01,  1.4304e-03,\n",
       "         5.0564e-06, -2.0265e-02,  1.8071e-03,  3.9894e-02, -1.6781e-02,\n",
       "        -5.4186e-02,  3.8820e-03,  2.7464e-02,  1.3147e-02,  1.4897e-02,\n",
       "        -1.6983e-01,  1.7717e-01,  8.1838e-03, -2.6388e-02, -3.7626e-02,\n",
       "        -1.6113e-02, -1.0144e-01,  1.0222e-02, -3.4020e-02, -1.1018e-02,\n",
       "         7.3555e-03, -1.0258e-02, -2.7347e-01,  1.5249e-02,  5.2636e-02,\n",
       "         6.4223e-04,  6.5132e-02, -8.1182e-04,  1.4412e-03, -4.4191e-04,\n",
       "         2.1029e-02, -2.4566e-01, -1.3988e-01, -4.6577e-01,  3.6334e-02,\n",
       "        -6.6171e-02, -4.1375e-02,  5.4973e-02, -1.3299e-01, -2.4264e-04,\n",
       "         6.6476e-02,  3.9937e-02, -4.6953e-02,  8.7733e-03, -4.7802e-02,\n",
       "        -5.4151e-02, -6.2299e-02,  1.7543e-01,  1.7309e-02, -3.2986e-02,\n",
       "        -1.4771e-01,  7.5149e-02,  1.8600e-02, -8.3768e-04, -9.1698e-02,\n",
       "         1.9026e-01, -2.5741e-02,  1.7867e+00,  1.6684e-03,  5.4089e-02,\n",
       "         3.8138e-02,  1.8875e-04,  1.4175e-01,  5.8372e-02, -3.8368e-02,\n",
       "         6.6562e-04,  1.9230e-01,  6.1132e-04, -4.7834e-02, -7.0154e-02,\n",
       "        -9.7792e-02, -7.4693e-05,  3.4184e-02,  1.0455e-04, -9.0578e-04,\n",
       "        -5.7287e-02,  2.1080e-02, -6.7264e-02, -2.0861e+00, -5.2612e-04,\n",
       "        -2.5418e-02, -4.5974e-02, -1.4495e-02,  3.6783e-01, -3.3998e-02,\n",
       "        -2.0853e-04, -3.4028e-05,  2.2618e-04, -1.6242e-01,  1.2113e-01,\n",
       "        -3.4937e-02,  8.0477e-02,  3.1981e-05,  8.9568e-04, -1.5696e-01,\n",
       "        -1.6395e-01, -2.3563e-04,  5.0342e-02,  6.7715e-02, -8.6016e-02,\n",
       "         2.7723e-02,  3.4312e-01, -1.0099e-01, -1.5193e-02, -2.7900e-02,\n",
       "         4.8545e-01, -1.2283e-01,  8.8978e-02,  4.1124e-03, -2.2974e-02,\n",
       "        -1.3016e-04,  1.4964e-02, -9.2522e-02,  8.9006e-02,  9.9098e-02,\n",
       "        -6.7341e-02, -9.2141e-02, -1.0010e-02, -2.0091e-03,  1.1137e-01,\n",
       "         5.0177e-02,  7.3514e-02, -1.4612e-02, -1.1116e-04,  3.3379e-02,\n",
       "         5.9744e-02,  1.3267e-02, -9.7211e-02, -1.1244e-01,  6.4597e-02,\n",
       "         1.9100e-01,  5.6448e-02,  1.9500e-02,  1.7671e-01,  2.1121e-02,\n",
       "         3.1953e-02,  1.6338e-01,  1.0491e-02,  5.8425e-01, -1.3744e-02,\n",
       "         4.8282e-01, -1.2161e-01, -2.2541e-02,  1.1786e-01,  9.4005e-05,\n",
       "        -4.6179e-01,  3.1077e-02,  1.7508e-02,  1.1067e-01, -1.3278e-03,\n",
       "         5.7325e-05, -1.7609e-01, -6.7496e-02, -1.9219e-02, -4.4213e-02,\n",
       "         9.1958e-02,  7.5989e-02,  2.3059e-01,  1.7753e-04,  1.5217e-01,\n",
       "         4.3717e-03, -1.0271e-01, -4.0095e-02,  7.7450e-02,  4.1740e-02,\n",
       "        -3.8876e-02, -2.0484e-03,  2.0108e-04,  4.3933e-02,  4.7804e-04,\n",
       "        -1.1029e-01,  3.7642e-04, -1.6913e-03,  4.0436e-02, -2.8519e-01,\n",
       "        -4.4282e-01,  1.0748e-01,  3.8162e-01, -3.4100e-02,  6.3007e-01,\n",
       "        -1.7541e-01,  4.5593e-02,  2.4163e-02,  1.7527e-01, -1.2842e-01,\n",
       "         1.5137e-01,  1.7252e-01,  6.1352e-04, -6.3998e-04, -5.5143e-02,\n",
       "         5.6069e-01,  1.0712e-01,  8.7701e-04,  8.7086e-03, -1.1627e-01,\n",
       "         3.1358e-01, -1.0722e-01, -4.3839e-04, -8.0088e-03, -1.0285e-02,\n",
       "         3.7319e-01,  2.7567e-02,  6.4857e-02,  1.1539e-01,  5.1639e-02,\n",
       "        -2.9165e-01, -7.9975e-04, -9.7061e-01,  2.9988e-01,  2.5472e-02,\n",
       "        -7.0909e-03, -4.5520e-02,  8.6347e-02, -5.2869e-02,  1.2896e-03,\n",
       "         1.1563e-01,  3.2400e-04,  1.1606e-01, -4.6152e-02,  1.0364e-01,\n",
       "        -2.9284e-04,  2.4769e-02,  1.2567e-01,  1.1983e-01, -1.4750e-02,\n",
       "        -1.2894e+00, -2.2015e-03,  4.5074e-02, -1.9746e-04,  3.2978e-02,\n",
       "        -1.5909e-01, -1.2976e-01, -2.0020e-02,  3.5568e-02,  4.7378e-01,\n",
       "         6.8227e-02,  2.8924e-02,  1.6270e-01,  2.5004e-02,  4.7461e-02,\n",
       "        -2.6924e-02,  7.8549e-02, -6.5731e-04,  1.4709e-04, -1.3571e-01,\n",
       "         1.1928e-03, -5.6484e-02,  7.6393e-02, -6.6513e-02, -2.9111e-01,\n",
       "         4.7720e-02,  8.4353e-02, -8.3931e-02,  3.8141e-02, -1.0380e-01,\n",
       "        -7.4313e-04,  2.9723e-02,  1.9012e-01, -4.9296e-02,  9.9830e-04,\n",
       "        -1.3475e-01,  2.9401e-04,  9.6349e-02, -1.5837e-02, -1.1177e-01,\n",
       "        -3.5058e-02, -7.2973e-02, -1.4043e-02,  1.7915e-03,  4.6373e-02,\n",
       "         1.6039e-02, -9.9526e-03,  2.2091e-01,  7.9054e-04, -1.5490e-01,\n",
       "         8.9916e-02,  2.7240e-01,  1.3426e-01, -1.8626e-01,  1.7941e-05,\n",
       "        -5.7518e-02, -4.7725e-02, -1.4364e-01,  3.3423e-02,  4.5165e-04,\n",
       "         1.5011e-01, -8.2927e-03,  9.6641e-04,  8.6386e-02, -1.0644e-01,\n",
       "         5.1394e-02,  3.0027e-02,  2.2067e-02,  2.7672e-01,  1.1513e-02,\n",
       "         1.6543e-01,  1.0307e-01, -7.2750e-02,  3.6094e-02,  9.8661e-03,\n",
       "        -1.1632e-01, -1.3388e-01,  2.9490e-03, -3.8642e-02,  4.3021e-02,\n",
       "        -2.9116e-01,  1.1169e-01,  5.1432e-02,  4.0985e-01,  4.1493e-02,\n",
       "         8.3552e-02, -1.0815e-02, -1.1575e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_state_dict[\"visual.transformer.resblocks.0.ln_1.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f38f893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.4004e-04,  1.5547e-02,  1.1443e-04, -2.5516e-04,  3.7344e-02,\n",
       "         6.8169e-02, -2.2151e-01,  1.2305e-01, -4.7701e-05,  4.3652e-02,\n",
       "        -1.3883e-01,  2.2224e-02,  3.1400e-03, -2.6446e-01, -1.9172e-04,\n",
       "         7.2726e-02,  9.1304e-02, -6.9150e-02,  6.0655e-04, -6.4259e-02,\n",
       "        -2.6761e-01, -6.3850e-02, -1.3749e-01, -2.0366e-02, -1.1364e-01,\n",
       "        -4.0331e-02,  7.0269e-02, -4.3323e-02,  3.5753e-01, -2.8951e-02,\n",
       "        -9.9992e-02, -8.4994e-04, -2.4468e-02, -5.2385e-02, -3.8524e-02,\n",
       "         6.3792e-04, -4.4348e-03,  1.7679e-01,  1.4868e-01,  3.4615e-01,\n",
       "         1.6235e-01, -7.9912e-02,  1.0218e-01,  4.7762e-02,  1.1078e-05,\n",
       "         1.8023e-03,  9.5139e-02, -5.1023e-02,  4.8207e-03,  1.8138e-02,\n",
       "         2.5796e-03,  7.8956e-02, -1.2881e-01, -2.5910e-02, -5.2114e-03,\n",
       "         8.1150e-02, -1.9824e-02,  2.1170e-05,  4.8710e-03, -2.7062e-01,\n",
       "         1.3506e-02,  3.9497e-02,  1.2379e-02,  2.1648e-02, -2.7365e-02,\n",
       "         2.0319e-01, -6.3227e-02, -5.1607e-02,  1.7295e-02,  1.3664e-01,\n",
       "        -6.6649e-02, -2.2080e-04,  1.5750e-02, -2.3084e-02,  1.0495e-03,\n",
       "        -9.8622e-02, -2.1366e-02,  3.4277e-05, -2.5390e-02, -1.1292e-01,\n",
       "        -9.8050e-02, -1.8265e-04,  6.1661e-02,  8.7809e-03, -5.6602e-02,\n",
       "         1.1661e-04,  2.2367e-02,  1.1256e-01,  1.2226e-01,  2.0021e-03,\n",
       "        -4.5702e-03,  1.6678e-01,  7.1395e-02,  5.1393e-04, -4.3480e-02,\n",
       "        -4.1367e-02,  4.4709e-04,  5.4882e-02, -1.3758e-01, -1.4060e-04,\n",
       "         4.2128e-01,  6.4923e-04,  4.0555e-04, -9.8343e-02, -1.0764e-02,\n",
       "        -2.5830e-01,  1.4377e-01,  1.9013e-01, -1.5254e-02, -8.8506e-03,\n",
       "         1.1477e-02, -3.3598e-01, -1.6540e-04, -5.5699e-04,  4.9909e-02,\n",
       "         9.0955e-02, -8.3932e-05,  2.8427e-03,  2.3548e-02, -2.2977e-01,\n",
       "         6.2530e-03, -2.9120e-01,  2.4793e-02,  1.1321e-01, -1.7752e-01,\n",
       "         8.2360e-04, -1.2158e-03,  7.5567e-02,  1.5903e-01, -2.0287e-01,\n",
       "         1.7382e-02,  2.7978e-02,  1.8872e-01, -3.4206e-02,  2.5710e-02,\n",
       "        -3.1813e-01, -2.8135e-02, -1.7887e-02,  1.3268e-03, -7.1234e-02,\n",
       "        -3.9153e-01, -9.1991e-04,  2.2538e-01,  7.9559e-02,  8.6730e-02,\n",
       "         2.8904e-01, -2.8560e-01, -7.3606e-02, -1.5836e-02,  2.7400e-01,\n",
       "        -1.4882e-02, -6.8840e-03,  5.0094e-03, -1.4853e-01,  1.9250e-01,\n",
       "         8.4492e-02,  1.5955e-02, -1.5349e-01, -2.5036e-01, -1.3879e-01,\n",
       "        -5.7912e-04,  4.6501e-02, -1.7446e-02, -8.1824e-02,  4.8804e-04,\n",
       "        -1.7781e-01, -8.0029e-02,  8.0600e-02,  4.8528e-03, -2.0018e-02,\n",
       "         5.3650e-02,  6.6022e-02,  9.9316e-02,  1.3975e-01,  5.4099e-02,\n",
       "         5.9351e-02, -1.4506e-02,  5.7632e-04,  3.4466e-01,  1.2584e-01,\n",
       "         2.8570e-02, -1.3919e-01, -3.0227e-02,  2.1479e-01,  6.1539e-02,\n",
       "         4.1191e-01, -1.9534e-01, -1.1296e-01, -8.4605e-02,  3.2953e-02,\n",
       "        -4.4872e-02, -2.8180e-01,  1.8373e-01, -2.4052e-02,  8.5291e-02,\n",
       "         2.8671e-02,  6.8167e-02, -4.6095e-02, -1.7146e-03, -2.5384e-04,\n",
       "         3.0720e-01,  1.7533e-01, -2.5039e-03, -8.9430e-02,  1.6281e-03,\n",
       "        -9.2651e-02,  3.2987e-02, -1.7221e-05,  8.2233e-02,  3.0135e-02,\n",
       "         7.9699e-02,  2.7639e-02, -7.4402e-02, -1.4628e-01,  6.8674e-02,\n",
       "        -1.3812e-02, -1.4699e-01,  2.5234e-02, -1.9108e-01,  1.2735e-01,\n",
       "        -6.3571e-02, -3.2842e-01,  1.5928e-01,  1.0166e-01, -9.5491e-02,\n",
       "         1.8244e-02, -1.0157e-01,  1.5943e-01,  4.1023e-01,  1.4648e-03,\n",
       "         1.1488e-01,  2.0162e-01,  9.1368e-02,  1.9622e-04, -2.9316e-02,\n",
       "        -4.0671e-02,  1.0903e-01,  1.0954e-01, -1.5449e-01, -4.7697e-02,\n",
       "        -2.7791e-02, -7.9599e-03, -8.0061e-02, -3.6353e-03,  2.4927e-04,\n",
       "        -1.1275e-03, -7.7331e-02, -9.5923e-03,  5.8110e-02, -9.9101e-02,\n",
       "        -7.7977e-02,  3.7906e-01, -6.7343e-02,  8.8664e-02, -5.8153e-02,\n",
       "         7.3247e-03, -8.8239e-02,  2.1293e-01,  9.9175e-02,  1.2003e-02,\n",
       "         2.1518e-01,  7.9398e-02,  3.6135e-02, -5.0748e-03,  6.2041e-02,\n",
       "        -2.7146e-02,  5.7961e-04,  5.7335e-02, -1.8677e-02, -5.0348e-02,\n",
       "         4.3455e-01,  1.5110e-02, -1.0475e-01,  3.5692e-02,  1.9759e-01,\n",
       "         1.7842e-03, -4.1909e-05, -2.9352e-02, -2.8309e-01, -2.3182e-03,\n",
       "         1.5341e-02,  6.6213e-02, -7.4137e-02, -2.5314e-04,  9.8726e-04,\n",
       "         8.5236e-02, -6.7166e-02,  5.9791e-02, -9.1404e-02, -2.4568e-02,\n",
       "        -1.9090e-03, -9.7865e-04,  4.7338e-02,  1.0620e-01,  1.4780e-01,\n",
       "         7.6617e-04,  3.0129e-02, -9.8124e-04,  2.2512e-01, -5.2414e-04,\n",
       "         6.0470e-04,  8.4479e-02,  9.2111e-02,  5.0325e-04, -1.6954e-01,\n",
       "        -9.9732e-02,  1.5064e-01,  3.2428e-02,  6.5680e-02,  4.1545e-02,\n",
       "        -6.6156e-05, -9.0956e-04,  1.0056e-01, -3.5699e-02,  2.7285e-02,\n",
       "         6.4173e-02,  3.3491e-01, -9.7724e-02,  1.2449e-01, -9.8282e-04,\n",
       "         7.3098e-02, -6.4294e-02,  5.5671e-02,  2.5822e-02, -4.0224e-04,\n",
       "         9.6178e-01, -5.7480e-02,  3.7623e-01, -8.9061e-02,  1.2464e-01,\n",
       "         8.8725e-04, -1.1090e-01,  1.2189e-04, -2.1884e-01,  3.6156e-02,\n",
       "        -3.1405e-04,  2.2374e-03, -1.1618e-03, -1.3030e-01,  8.7413e-03,\n",
       "        -1.6229e-01, -1.9270e-02,  3.1752e-02, -1.0918e-01,  7.0276e-02,\n",
       "         6.1208e-02, -9.7448e-02, -1.1022e-01,  2.8551e-03, -7.4570e-02,\n",
       "         7.3743e-02, -1.0949e-01,  4.9151e-02, -1.1366e-03, -1.0124e-02,\n",
       "        -7.8827e-02,  9.0588e-04,  7.0769e-03,  6.7701e-02,  3.7486e-04,\n",
       "        -5.5932e-02, -9.9676e-02,  6.3592e-02,  1.8093e-01, -4.8160e-02,\n",
       "         1.0736e-01,  1.5293e-02, -3.0901e-01, -8.3350e-06,  4.0838e-04,\n",
       "        -2.1467e-01,  2.6399e-02, -2.6324e-04, -1.6416e-01,  2.2701e-02,\n",
       "        -2.6170e-04, -2.4388e-04,  4.4249e-02, -3.7792e-02, -4.0168e-02,\n",
       "         3.1859e-02, -1.8747e-01, -7.0880e-03,  2.3951e-02, -1.0346e-02,\n",
       "        -6.3756e-02, -1.0866e-01,  1.4680e-01,  4.8045e-02, -8.4171e-02,\n",
       "        -1.3456e-01, -1.1332e-01,  5.9682e-01,  3.4385e-02, -6.0843e-02,\n",
       "        -1.1704e-01,  2.4422e-02,  1.8723e-02,  1.7452e-01,  8.3422e-02,\n",
       "        -2.0591e-01, -2.1573e-02,  5.3128e-02, -7.7725e-02, -1.5428e-02,\n",
       "        -1.2198e-02,  1.2746e-01,  7.9414e-04, -1.8920e-02, -5.3817e-02,\n",
       "        -4.4739e-01,  7.2493e-02, -3.1372e-02, -1.2261e-01,  1.6727e-02,\n",
       "         4.7413e-02, -1.2631e-03, -9.4757e-02,  3.5834e-02, -3.3941e-02,\n",
       "        -3.4799e-02,  5.3317e-02,  1.6142e-01, -4.3102e-02, -9.3018e-02,\n",
       "         2.1222e-02,  4.8347e-02,  9.1428e-02,  2.6013e-01, -3.6606e-04,\n",
       "         2.1686e-04, -6.6778e-02, -2.4917e-02,  4.9077e-01,  7.0667e-02,\n",
       "        -2.2207e-03,  1.1292e-01,  1.5435e-01,  1.6694e-01, -7.2930e-02,\n",
       "         3.3292e-04, -1.4773e-01,  2.3937e-01,  4.8174e-04,  6.0665e-02,\n",
       "         8.8716e-02, -3.4184e-02,  1.4542e-01, -6.9053e-02, -5.7849e-02,\n",
       "        -9.2947e-02, -9.2739e-02,  1.8066e-01,  1.3084e-01,  1.4304e-03,\n",
       "         5.0564e-06, -2.0265e-02,  1.8071e-03,  3.9894e-02, -1.6781e-02,\n",
       "        -5.4186e-02,  3.8820e-03,  2.7464e-02,  1.3147e-02,  1.4897e-02,\n",
       "        -1.6983e-01,  1.7717e-01,  8.1838e-03, -2.6388e-02, -3.7626e-02,\n",
       "        -1.6113e-02, -1.0144e-01,  1.0222e-02, -3.4020e-02, -1.1018e-02,\n",
       "         7.3555e-03, -1.0258e-02, -2.7347e-01,  1.5249e-02,  5.2636e-02,\n",
       "         6.4223e-04,  6.5132e-02, -8.1182e-04,  1.4412e-03, -4.4191e-04,\n",
       "         2.1029e-02, -2.4566e-01, -1.3988e-01, -4.6577e-01,  3.6334e-02,\n",
       "        -6.6171e-02, -4.1375e-02,  5.4973e-02, -1.3299e-01, -2.4264e-04,\n",
       "         6.6476e-02,  3.9937e-02, -4.6953e-02,  8.7733e-03, -4.7802e-02,\n",
       "        -5.4151e-02, -6.2299e-02,  1.7543e-01,  1.7309e-02, -3.2986e-02,\n",
       "        -1.4771e-01,  7.5149e-02,  1.8600e-02, -8.3768e-04, -9.1698e-02,\n",
       "         1.9026e-01, -2.5741e-02,  1.7867e+00,  1.6684e-03,  5.4089e-02,\n",
       "         3.8138e-02,  1.8875e-04,  1.4175e-01,  5.8372e-02, -3.8368e-02,\n",
       "         6.6562e-04,  1.9230e-01,  6.1132e-04, -4.7834e-02, -7.0154e-02,\n",
       "        -9.7792e-02, -7.4693e-05,  3.4184e-02,  1.0455e-04, -9.0578e-04,\n",
       "        -5.7287e-02,  2.1080e-02, -6.7264e-02, -2.0861e+00, -5.2612e-04,\n",
       "        -2.5418e-02, -4.5974e-02, -1.4495e-02,  3.6783e-01, -3.3998e-02,\n",
       "        -2.0853e-04, -3.4028e-05,  2.2618e-04, -1.6242e-01,  1.2113e-01,\n",
       "        -3.4937e-02,  8.0477e-02,  3.1981e-05,  8.9568e-04, -1.5696e-01,\n",
       "        -1.6395e-01, -2.3563e-04,  5.0342e-02,  6.7715e-02, -8.6016e-02,\n",
       "         2.7723e-02,  3.4312e-01, -1.0099e-01, -1.5193e-02, -2.7900e-02,\n",
       "         4.8545e-01, -1.2283e-01,  8.8978e-02,  4.1124e-03, -2.2974e-02,\n",
       "        -1.3016e-04,  1.4964e-02, -9.2522e-02,  8.9006e-02,  9.9098e-02,\n",
       "        -6.7341e-02, -9.2141e-02, -1.0010e-02, -2.0091e-03,  1.1137e-01,\n",
       "         5.0177e-02,  7.3514e-02, -1.4612e-02, -1.1116e-04,  3.3379e-02,\n",
       "         5.9744e-02,  1.3267e-02, -9.7211e-02, -1.1244e-01,  6.4597e-02,\n",
       "         1.9100e-01,  5.6448e-02,  1.9500e-02,  1.7671e-01,  2.1121e-02,\n",
       "         3.1953e-02,  1.6338e-01,  1.0491e-02,  5.8425e-01, -1.3744e-02,\n",
       "         4.8282e-01, -1.2161e-01, -2.2541e-02,  1.1786e-01,  9.4005e-05,\n",
       "        -4.6179e-01,  3.1077e-02,  1.7508e-02,  1.1067e-01, -1.3278e-03,\n",
       "         5.7325e-05, -1.7609e-01, -6.7496e-02, -1.9219e-02, -4.4213e-02,\n",
       "         9.1958e-02,  7.5989e-02,  2.3059e-01,  1.7753e-04,  1.5217e-01,\n",
       "         4.3717e-03, -1.0271e-01, -4.0095e-02,  7.7450e-02,  4.1740e-02,\n",
       "        -3.8876e-02, -2.0484e-03,  2.0108e-04,  4.3933e-02,  4.7804e-04,\n",
       "        -1.1029e-01,  3.7642e-04, -1.6913e-03,  4.0436e-02, -2.8519e-01,\n",
       "        -4.4282e-01,  1.0748e-01,  3.8162e-01, -3.4100e-02,  6.3007e-01,\n",
       "        -1.7541e-01,  4.5593e-02,  2.4163e-02,  1.7527e-01, -1.2842e-01,\n",
       "         1.5137e-01,  1.7252e-01,  6.1352e-04, -6.3998e-04, -5.5143e-02,\n",
       "         5.6069e-01,  1.0712e-01,  8.7701e-04,  8.7086e-03, -1.1627e-01,\n",
       "         3.1358e-01, -1.0722e-01, -4.3839e-04, -8.0088e-03, -1.0285e-02,\n",
       "         3.7319e-01,  2.7567e-02,  6.4857e-02,  1.1539e-01,  5.1639e-02,\n",
       "        -2.9165e-01, -7.9975e-04, -9.7061e-01,  2.9988e-01,  2.5472e-02,\n",
       "        -7.0909e-03, -4.5520e-02,  8.6347e-02, -5.2869e-02,  1.2896e-03,\n",
       "         1.1563e-01,  3.2400e-04,  1.1606e-01, -4.6152e-02,  1.0364e-01,\n",
       "        -2.9284e-04,  2.4769e-02,  1.2567e-01,  1.1983e-01, -1.4750e-02,\n",
       "        -1.2894e+00, -2.2015e-03,  4.5074e-02, -1.9746e-04,  3.2978e-02,\n",
       "        -1.5909e-01, -1.2976e-01, -2.0020e-02,  3.5568e-02,  4.7378e-01,\n",
       "         6.8227e-02,  2.8924e-02,  1.6270e-01,  2.5004e-02,  4.7461e-02,\n",
       "        -2.6924e-02,  7.8549e-02, -6.5731e-04,  1.4709e-04, -1.3571e-01,\n",
       "         1.1928e-03, -5.6484e-02,  7.6393e-02, -6.6513e-02, -2.9111e-01,\n",
       "         4.7720e-02,  8.4353e-02, -8.3931e-02,  3.8141e-02, -1.0380e-01,\n",
       "        -7.4313e-04,  2.9723e-02,  1.9012e-01, -4.9296e-02,  9.9830e-04,\n",
       "        -1.3475e-01,  2.9401e-04,  9.6349e-02, -1.5837e-02, -1.1177e-01,\n",
       "        -3.5058e-02, -7.2973e-02, -1.4043e-02,  1.7915e-03,  4.6373e-02,\n",
       "         1.6039e-02, -9.9526e-03,  2.2091e-01,  7.9054e-04, -1.5490e-01,\n",
       "         8.9916e-02,  2.7240e-01,  1.3426e-01, -1.8626e-01,  1.7941e-05,\n",
       "        -5.7518e-02, -4.7725e-02, -1.4364e-01,  3.3423e-02,  4.5165e-04,\n",
       "         1.5011e-01, -8.2927e-03,  9.6641e-04,  8.6386e-02, -1.0644e-01,\n",
       "         5.1394e-02,  3.0027e-02,  2.2067e-02,  2.7672e-01,  1.1513e-02,\n",
       "         1.6543e-01,  1.0307e-01, -7.2750e-02,  3.6094e-02,  9.8661e-03,\n",
       "        -1.1632e-01, -1.3388e-01,  2.9490e-03, -3.8642e-02,  4.3021e-02,\n",
       "        -2.9116e-01,  1.1169e-01,  5.1432e-02,  4.0985e-01,  4.1493e-02,\n",
       "         8.3552e-02, -1.0815e-02, -1.1575e-01], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_state_dict['blocks.0.ln1.b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7114127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "embed.proj.weight\n",
      "embed.proj.bias\n",
      "pos_embed.W_pos\n",
      "ln_pre.w\n",
      "ln_pre.b\n",
      "blocks.0.ln1.w\n",
      "blocks.0.ln1.b\n",
      "blocks.0.ln2.w\n",
      "blocks.0.ln2.b\n",
      "blocks.0.attn.W_Q\n",
      "blocks.0.attn.W_K\n",
      "blocks.0.attn.W_V\n",
      "blocks.0.attn.W_O\n",
      "blocks.0.attn.b_Q\n",
      "blocks.0.attn.b_K\n",
      "blocks.0.attn.b_V\n",
      "blocks.0.attn.b_O\n",
      "blocks.0.mlp.W_in\n",
      "blocks.0.mlp.b_in\n",
      "blocks.0.mlp.W_out\n",
      "blocks.0.mlp.b_out\n",
      "blocks.1.ln1.w\n",
      "blocks.1.ln1.b\n",
      "blocks.1.ln2.w\n",
      "blocks.1.ln2.b\n",
      "blocks.1.attn.W_Q\n",
      "blocks.1.attn.W_K\n",
      "blocks.1.attn.W_V\n",
      "blocks.1.attn.W_O\n",
      "blocks.1.attn.b_Q\n",
      "blocks.1.attn.b_K\n",
      "blocks.1.attn.b_V\n",
      "blocks.1.attn.b_O\n",
      "blocks.1.mlp.W_in\n",
      "blocks.1.mlp.b_in\n",
      "blocks.1.mlp.W_out\n",
      "blocks.1.mlp.b_out\n",
      "blocks.2.ln1.w\n",
      "blocks.2.ln1.b\n",
      "blocks.2.ln2.w\n",
      "blocks.2.ln2.b\n",
      "blocks.2.attn.W_Q\n",
      "blocks.2.attn.W_K\n",
      "blocks.2.attn.W_V\n",
      "blocks.2.attn.W_O\n",
      "blocks.2.attn.b_Q\n",
      "blocks.2.attn.b_K\n",
      "blocks.2.attn.b_V\n",
      "blocks.2.attn.b_O\n",
      "blocks.2.mlp.W_in\n",
      "blocks.2.mlp.b_in\n",
      "blocks.2.mlp.W_out\n",
      "blocks.2.mlp.b_out\n",
      "blocks.3.ln1.w\n",
      "blocks.3.ln1.b\n",
      "blocks.3.ln2.w\n",
      "blocks.3.ln2.b\n",
      "blocks.3.attn.W_Q\n",
      "blocks.3.attn.W_K\n",
      "blocks.3.attn.W_V\n",
      "blocks.3.attn.W_O\n",
      "blocks.3.attn.b_Q\n",
      "blocks.3.attn.b_K\n",
      "blocks.3.attn.b_V\n",
      "blocks.3.attn.b_O\n",
      "blocks.3.mlp.W_in\n",
      "blocks.3.mlp.b_in\n",
      "blocks.3.mlp.W_out\n",
      "blocks.3.mlp.b_out\n",
      "blocks.4.ln1.w\n",
      "blocks.4.ln1.b\n",
      "blocks.4.ln2.w\n",
      "blocks.4.ln2.b\n",
      "blocks.4.attn.W_Q\n",
      "blocks.4.attn.W_K\n",
      "blocks.4.attn.W_V\n",
      "blocks.4.attn.W_O\n",
      "blocks.4.attn.b_Q\n",
      "blocks.4.attn.b_K\n",
      "blocks.4.attn.b_V\n",
      "blocks.4.attn.b_O\n",
      "blocks.4.mlp.W_in\n",
      "blocks.4.mlp.b_in\n",
      "blocks.4.mlp.W_out\n",
      "blocks.4.mlp.b_out\n",
      "blocks.5.ln1.w\n",
      "blocks.5.ln1.b\n",
      "blocks.5.ln2.w\n",
      "blocks.5.ln2.b\n",
      "blocks.5.attn.W_Q\n",
      "blocks.5.attn.W_K\n",
      "blocks.5.attn.W_V\n",
      "blocks.5.attn.W_O\n",
      "blocks.5.attn.b_Q\n",
      "blocks.5.attn.b_K\n",
      "blocks.5.attn.b_V\n",
      "blocks.5.attn.b_O\n",
      "blocks.5.mlp.W_in\n",
      "blocks.5.mlp.b_in\n",
      "blocks.5.mlp.W_out\n",
      "blocks.5.mlp.b_out\n",
      "blocks.6.ln1.w\n",
      "blocks.6.ln1.b\n",
      "blocks.6.ln2.w\n",
      "blocks.6.ln2.b\n",
      "blocks.6.attn.W_Q\n",
      "blocks.6.attn.W_K\n",
      "blocks.6.attn.W_V\n",
      "blocks.6.attn.W_O\n",
      "blocks.6.attn.b_Q\n",
      "blocks.6.attn.b_K\n",
      "blocks.6.attn.b_V\n",
      "blocks.6.attn.b_O\n",
      "blocks.6.mlp.W_in\n",
      "blocks.6.mlp.b_in\n",
      "blocks.6.mlp.W_out\n",
      "blocks.6.mlp.b_out\n",
      "blocks.7.ln1.w\n",
      "blocks.7.ln1.b\n",
      "blocks.7.ln2.w\n",
      "blocks.7.ln2.b\n",
      "blocks.7.attn.W_Q\n",
      "blocks.7.attn.W_K\n",
      "blocks.7.attn.W_V\n",
      "blocks.7.attn.W_O\n",
      "blocks.7.attn.b_Q\n",
      "blocks.7.attn.b_K\n",
      "blocks.7.attn.b_V\n",
      "blocks.7.attn.b_O\n",
      "blocks.7.mlp.W_in\n",
      "blocks.7.mlp.b_in\n",
      "blocks.7.mlp.W_out\n",
      "blocks.7.mlp.b_out\n",
      "blocks.8.ln1.w\n",
      "blocks.8.ln1.b\n",
      "blocks.8.ln2.w\n",
      "blocks.8.ln2.b\n",
      "blocks.8.attn.W_Q\n",
      "blocks.8.attn.W_K\n",
      "blocks.8.attn.W_V\n",
      "blocks.8.attn.W_O\n",
      "blocks.8.attn.b_Q\n",
      "blocks.8.attn.b_K\n",
      "blocks.8.attn.b_V\n",
      "blocks.8.attn.b_O\n",
      "blocks.8.mlp.W_in\n",
      "blocks.8.mlp.b_in\n",
      "blocks.8.mlp.W_out\n",
      "blocks.8.mlp.b_out\n",
      "blocks.9.ln1.w\n",
      "blocks.9.ln1.b\n",
      "blocks.9.ln2.w\n",
      "blocks.9.ln2.b\n",
      "blocks.9.attn.W_Q\n",
      "blocks.9.attn.W_K\n",
      "blocks.9.attn.W_V\n",
      "blocks.9.attn.W_O\n",
      "blocks.9.attn.b_Q\n",
      "blocks.9.attn.b_K\n",
      "blocks.9.attn.b_V\n",
      "blocks.9.attn.b_O\n",
      "blocks.9.mlp.W_in\n",
      "blocks.9.mlp.b_in\n",
      "blocks.9.mlp.W_out\n",
      "blocks.9.mlp.b_out\n",
      "blocks.10.ln1.w\n",
      "blocks.10.ln1.b\n",
      "blocks.10.ln2.w\n",
      "blocks.10.ln2.b\n",
      "blocks.10.attn.W_Q\n",
      "blocks.10.attn.W_K\n",
      "blocks.10.attn.W_V\n",
      "blocks.10.attn.W_O\n",
      "blocks.10.attn.b_Q\n",
      "blocks.10.attn.b_K\n",
      "blocks.10.attn.b_V\n",
      "blocks.10.attn.b_O\n",
      "blocks.10.mlp.W_in\n",
      "blocks.10.mlp.b_in\n",
      "blocks.10.mlp.W_out\n",
      "blocks.10.mlp.b_out\n",
      "blocks.11.ln1.w\n",
      "blocks.11.ln1.b\n",
      "blocks.11.ln2.w\n",
      "blocks.11.ln2.b\n",
      "blocks.11.attn.W_Q\n",
      "blocks.11.attn.W_K\n",
      "blocks.11.attn.W_V\n",
      "blocks.11.attn.W_O\n",
      "blocks.11.attn.b_Q\n",
      "blocks.11.attn.b_K\n",
      "blocks.11.attn.b_V\n",
      "blocks.11.attn.b_O\n",
      "blocks.11.mlp.W_in\n",
      "blocks.11.mlp.b_in\n",
      "blocks.11.mlp.W_out\n",
      "blocks.11.mlp.b_out\n",
      "ln_final.w\n",
      "ln_final.b\n",
      "head.W_H\n",
      "head.b_H\n"
     ]
    }
   ],
   "source": [
    "for k, v in hooked_state_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734d702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e4ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_embedding\n",
      "text_projection\n",
      "logit_scale\n",
      "visual.class_embedding\n",
      "visual.positional_embedding\n",
      "visual.proj\n",
      "visual.conv1.weight\n",
      "visual.ln_pre.weight\n",
      "visual.ln_pre.bias\n",
      "visual.transformer.resblocks.0.ln_1.weight\n",
      "visual.transformer.resblocks.0.ln_1.bias\n",
      "visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "visual.transformer.resblocks.0.ln_2.weight\n",
      "visual.transformer.resblocks.0.ln_2.bias\n",
      "visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.1.ln_1.weight\n",
      "visual.transformer.resblocks.1.ln_1.bias\n",
      "visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "visual.transformer.resblocks.1.ln_2.weight\n",
      "visual.transformer.resblocks.1.ln_2.bias\n",
      "visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.2.ln_1.weight\n",
      "visual.transformer.resblocks.2.ln_1.bias\n",
      "visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "visual.transformer.resblocks.2.ln_2.weight\n",
      "visual.transformer.resblocks.2.ln_2.bias\n",
      "visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.3.ln_1.weight\n",
      "visual.transformer.resblocks.3.ln_1.bias\n",
      "visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "visual.transformer.resblocks.3.ln_2.weight\n",
      "visual.transformer.resblocks.3.ln_2.bias\n",
      "visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.4.ln_1.weight\n",
      "visual.transformer.resblocks.4.ln_1.bias\n",
      "visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "visual.transformer.resblocks.4.ln_2.weight\n",
      "visual.transformer.resblocks.4.ln_2.bias\n",
      "visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.5.ln_1.weight\n",
      "visual.transformer.resblocks.5.ln_1.bias\n",
      "visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "visual.transformer.resblocks.5.ln_2.weight\n",
      "visual.transformer.resblocks.5.ln_2.bias\n",
      "visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.6.ln_1.weight\n",
      "visual.transformer.resblocks.6.ln_1.bias\n",
      "visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "visual.transformer.resblocks.6.ln_2.weight\n",
      "visual.transformer.resblocks.6.ln_2.bias\n",
      "visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.7.ln_1.weight\n",
      "visual.transformer.resblocks.7.ln_1.bias\n",
      "visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "visual.transformer.resblocks.7.ln_2.weight\n",
      "visual.transformer.resblocks.7.ln_2.bias\n",
      "visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.8.ln_1.weight\n",
      "visual.transformer.resblocks.8.ln_1.bias\n",
      "visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "visual.transformer.resblocks.8.ln_2.weight\n",
      "visual.transformer.resblocks.8.ln_2.bias\n",
      "visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.9.ln_1.weight\n",
      "visual.transformer.resblocks.9.ln_1.bias\n",
      "visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "visual.transformer.resblocks.9.ln_2.weight\n",
      "visual.transformer.resblocks.9.ln_2.bias\n",
      "visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.10.ln_1.weight\n",
      "visual.transformer.resblocks.10.ln_1.bias\n",
      "visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "visual.transformer.resblocks.10.ln_2.weight\n",
      "visual.transformer.resblocks.10.ln_2.bias\n",
      "visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.11.ln_1.weight\n",
      "visual.transformer.resblocks.11.ln_1.bias\n",
      "visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "visual.transformer.resblocks.11.ln_2.weight\n",
      "visual.transformer.resblocks.11.ln_2.bias\n",
      "visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "visual.ln_post.weight\n",
      "visual.ln_post.bias\n",
      "transformer.resblocks.0.ln_1.weight\n",
      "transformer.resblocks.0.ln_1.bias\n",
      "transformer.resblocks.0.attn.in_proj_weight\n",
      "transformer.resblocks.0.attn.in_proj_bias\n",
      "transformer.resblocks.0.attn.out_proj.weight\n",
      "transformer.resblocks.0.attn.out_proj.bias\n",
      "transformer.resblocks.0.ln_2.weight\n",
      "transformer.resblocks.0.ln_2.bias\n",
      "transformer.resblocks.0.mlp.c_fc.weight\n",
      "transformer.resblocks.0.mlp.c_fc.bias\n",
      "transformer.resblocks.0.mlp.c_proj.weight\n",
      "transformer.resblocks.0.mlp.c_proj.bias\n",
      "transformer.resblocks.1.ln_1.weight\n",
      "transformer.resblocks.1.ln_1.bias\n",
      "transformer.resblocks.1.attn.in_proj_weight\n",
      "transformer.resblocks.1.attn.in_proj_bias\n",
      "transformer.resblocks.1.attn.out_proj.weight\n",
      "transformer.resblocks.1.attn.out_proj.bias\n",
      "transformer.resblocks.1.ln_2.weight\n",
      "transformer.resblocks.1.ln_2.bias\n",
      "transformer.resblocks.1.mlp.c_fc.weight\n",
      "transformer.resblocks.1.mlp.c_fc.bias\n",
      "transformer.resblocks.1.mlp.c_proj.weight\n",
      "transformer.resblocks.1.mlp.c_proj.bias\n",
      "transformer.resblocks.2.ln_1.weight\n",
      "transformer.resblocks.2.ln_1.bias\n",
      "transformer.resblocks.2.attn.in_proj_weight\n",
      "transformer.resblocks.2.attn.in_proj_bias\n",
      "transformer.resblocks.2.attn.out_proj.weight\n",
      "transformer.resblocks.2.attn.out_proj.bias\n",
      "transformer.resblocks.2.ln_2.weight\n",
      "transformer.resblocks.2.ln_2.bias\n",
      "transformer.resblocks.2.mlp.c_fc.weight\n",
      "transformer.resblocks.2.mlp.c_fc.bias\n",
      "transformer.resblocks.2.mlp.c_proj.weight\n",
      "transformer.resblocks.2.mlp.c_proj.bias\n",
      "transformer.resblocks.3.ln_1.weight\n",
      "transformer.resblocks.3.ln_1.bias\n",
      "transformer.resblocks.3.attn.in_proj_weight\n",
      "transformer.resblocks.3.attn.in_proj_bias\n",
      "transformer.resblocks.3.attn.out_proj.weight\n",
      "transformer.resblocks.3.attn.out_proj.bias\n",
      "transformer.resblocks.3.ln_2.weight\n",
      "transformer.resblocks.3.ln_2.bias\n",
      "transformer.resblocks.3.mlp.c_fc.weight\n",
      "transformer.resblocks.3.mlp.c_fc.bias\n",
      "transformer.resblocks.3.mlp.c_proj.weight\n",
      "transformer.resblocks.3.mlp.c_proj.bias\n",
      "transformer.resblocks.4.ln_1.weight\n",
      "transformer.resblocks.4.ln_1.bias\n",
      "transformer.resblocks.4.attn.in_proj_weight\n",
      "transformer.resblocks.4.attn.in_proj_bias\n",
      "transformer.resblocks.4.attn.out_proj.weight\n",
      "transformer.resblocks.4.attn.out_proj.bias\n",
      "transformer.resblocks.4.ln_2.weight\n",
      "transformer.resblocks.4.ln_2.bias\n",
      "transformer.resblocks.4.mlp.c_fc.weight\n",
      "transformer.resblocks.4.mlp.c_fc.bias\n",
      "transformer.resblocks.4.mlp.c_proj.weight\n",
      "transformer.resblocks.4.mlp.c_proj.bias\n",
      "transformer.resblocks.5.ln_1.weight\n",
      "transformer.resblocks.5.ln_1.bias\n",
      "transformer.resblocks.5.attn.in_proj_weight\n",
      "transformer.resblocks.5.attn.in_proj_bias\n",
      "transformer.resblocks.5.attn.out_proj.weight\n",
      "transformer.resblocks.5.attn.out_proj.bias\n",
      "transformer.resblocks.5.ln_2.weight\n",
      "transformer.resblocks.5.ln_2.bias\n",
      "transformer.resblocks.5.mlp.c_fc.weight\n",
      "transformer.resblocks.5.mlp.c_fc.bias\n",
      "transformer.resblocks.5.mlp.c_proj.weight\n",
      "transformer.resblocks.5.mlp.c_proj.bias\n",
      "transformer.resblocks.6.ln_1.weight\n",
      "transformer.resblocks.6.ln_1.bias\n",
      "transformer.resblocks.6.attn.in_proj_weight\n",
      "transformer.resblocks.6.attn.in_proj_bias\n",
      "transformer.resblocks.6.attn.out_proj.weight\n",
      "transformer.resblocks.6.attn.out_proj.bias\n",
      "transformer.resblocks.6.ln_2.weight\n",
      "transformer.resblocks.6.ln_2.bias\n",
      "transformer.resblocks.6.mlp.c_fc.weight\n",
      "transformer.resblocks.6.mlp.c_fc.bias\n",
      "transformer.resblocks.6.mlp.c_proj.weight\n",
      "transformer.resblocks.6.mlp.c_proj.bias\n",
      "transformer.resblocks.7.ln_1.weight\n",
      "transformer.resblocks.7.ln_1.bias\n",
      "transformer.resblocks.7.attn.in_proj_weight\n",
      "transformer.resblocks.7.attn.in_proj_bias\n",
      "transformer.resblocks.7.attn.out_proj.weight\n",
      "transformer.resblocks.7.attn.out_proj.bias\n",
      "transformer.resblocks.7.ln_2.weight\n",
      "transformer.resblocks.7.ln_2.bias\n",
      "transformer.resblocks.7.mlp.c_fc.weight\n",
      "transformer.resblocks.7.mlp.c_fc.bias\n",
      "transformer.resblocks.7.mlp.c_proj.weight\n",
      "transformer.resblocks.7.mlp.c_proj.bias\n",
      "transformer.resblocks.8.ln_1.weight\n",
      "transformer.resblocks.8.ln_1.bias\n",
      "transformer.resblocks.8.attn.in_proj_weight\n",
      "transformer.resblocks.8.attn.in_proj_bias\n",
      "transformer.resblocks.8.attn.out_proj.weight\n",
      "transformer.resblocks.8.attn.out_proj.bias\n",
      "transformer.resblocks.8.ln_2.weight\n",
      "transformer.resblocks.8.ln_2.bias\n",
      "transformer.resblocks.8.mlp.c_fc.weight\n",
      "transformer.resblocks.8.mlp.c_fc.bias\n",
      "transformer.resblocks.8.mlp.c_proj.weight\n",
      "transformer.resblocks.8.mlp.c_proj.bias\n",
      "transformer.resblocks.9.ln_1.weight\n",
      "transformer.resblocks.9.ln_1.bias\n",
      "transformer.resblocks.9.attn.in_proj_weight\n",
      "transformer.resblocks.9.attn.in_proj_bias\n",
      "transformer.resblocks.9.attn.out_proj.weight\n",
      "transformer.resblocks.9.attn.out_proj.bias\n",
      "transformer.resblocks.9.ln_2.weight\n",
      "transformer.resblocks.9.ln_2.bias\n",
      "transformer.resblocks.9.mlp.c_fc.weight\n",
      "transformer.resblocks.9.mlp.c_fc.bias\n",
      "transformer.resblocks.9.mlp.c_proj.weight\n",
      "transformer.resblocks.9.mlp.c_proj.bias\n",
      "transformer.resblocks.10.ln_1.weight\n",
      "transformer.resblocks.10.ln_1.bias\n",
      "transformer.resblocks.10.attn.in_proj_weight\n",
      "transformer.resblocks.10.attn.in_proj_bias\n",
      "transformer.resblocks.10.attn.out_proj.weight\n",
      "transformer.resblocks.10.attn.out_proj.bias\n",
      "transformer.resblocks.10.ln_2.weight\n",
      "transformer.resblocks.10.ln_2.bias\n",
      "transformer.resblocks.10.mlp.c_fc.weight\n",
      "transformer.resblocks.10.mlp.c_fc.bias\n",
      "transformer.resblocks.10.mlp.c_proj.weight\n",
      "transformer.resblocks.10.mlp.c_proj.bias\n",
      "transformer.resblocks.11.ln_1.weight\n",
      "transformer.resblocks.11.ln_1.bias\n",
      "transformer.resblocks.11.attn.in_proj_weight\n",
      "transformer.resblocks.11.attn.in_proj_bias\n",
      "transformer.resblocks.11.attn.out_proj.weight\n",
      "transformer.resblocks.11.attn.out_proj.bias\n",
      "transformer.resblocks.11.ln_2.weight\n",
      "transformer.resblocks.11.ln_2.bias\n",
      "transformer.resblocks.11.mlp.c_fc.weight\n",
      "transformer.resblocks.11.mlp.c_fc.bias\n",
      "transformer.resblocks.11.mlp.c_proj.weight\n",
      "transformer.resblocks.11.mlp.c_proj.bias\n",
      "token_embedding.weight\n",
      "ln_final.weight\n",
      "ln_final.bias\n"
     ]
    }
   ],
   "source": [
    "for k, v in og_state_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43a3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "embed.proj.weight\n",
      "embed.proj.bias\n",
      "pos_embed.W_pos\n",
      "ln_pre.w\n",
      "ln_pre.b\n",
      "blocks.0.ln1.w\n",
      "blocks.0.ln1.b\n",
      "blocks.0.ln2.w\n",
      "blocks.0.ln2.b\n",
      "blocks.0.attn.W_Q\n",
      "blocks.0.attn.W_K\n",
      "blocks.0.attn.W_V\n",
      "blocks.0.attn.W_O\n",
      "blocks.0.attn.b_Q\n",
      "blocks.0.attn.b_K\n",
      "blocks.0.attn.b_V\n",
      "blocks.0.attn.b_O\n",
      "blocks.0.mlp.W_in\n",
      "blocks.0.mlp.b_in\n",
      "blocks.0.mlp.W_out\n",
      "blocks.0.mlp.b_out\n",
      "blocks.1.ln1.w\n",
      "blocks.1.ln1.b\n",
      "blocks.1.ln2.w\n",
      "blocks.1.ln2.b\n",
      "blocks.1.attn.W_Q\n",
      "blocks.1.attn.W_K\n",
      "blocks.1.attn.W_V\n",
      "blocks.1.attn.W_O\n",
      "blocks.1.attn.b_Q\n",
      "blocks.1.attn.b_K\n",
      "blocks.1.attn.b_V\n",
      "blocks.1.attn.b_O\n",
      "blocks.1.mlp.W_in\n",
      "blocks.1.mlp.b_in\n",
      "blocks.1.mlp.W_out\n",
      "blocks.1.mlp.b_out\n",
      "blocks.2.ln1.w\n",
      "blocks.2.ln1.b\n",
      "blocks.2.ln2.w\n",
      "blocks.2.ln2.b\n",
      "blocks.2.attn.W_Q\n",
      "blocks.2.attn.W_K\n",
      "blocks.2.attn.W_V\n",
      "blocks.2.attn.W_O\n",
      "blocks.2.attn.b_Q\n",
      "blocks.2.attn.b_K\n",
      "blocks.2.attn.b_V\n",
      "blocks.2.attn.b_O\n",
      "blocks.2.mlp.W_in\n",
      "blocks.2.mlp.b_in\n",
      "blocks.2.mlp.W_out\n",
      "blocks.2.mlp.b_out\n",
      "blocks.3.ln1.w\n",
      "blocks.3.ln1.b\n",
      "blocks.3.ln2.w\n",
      "blocks.3.ln2.b\n",
      "blocks.3.attn.W_Q\n",
      "blocks.3.attn.W_K\n",
      "blocks.3.attn.W_V\n",
      "blocks.3.attn.W_O\n",
      "blocks.3.attn.b_Q\n",
      "blocks.3.attn.b_K\n",
      "blocks.3.attn.b_V\n",
      "blocks.3.attn.b_O\n",
      "blocks.3.mlp.W_in\n",
      "blocks.3.mlp.b_in\n",
      "blocks.3.mlp.W_out\n",
      "blocks.3.mlp.b_out\n",
      "blocks.4.ln1.w\n",
      "blocks.4.ln1.b\n",
      "blocks.4.ln2.w\n",
      "blocks.4.ln2.b\n",
      "blocks.4.attn.W_Q\n",
      "blocks.4.attn.W_K\n",
      "blocks.4.attn.W_V\n",
      "blocks.4.attn.W_O\n",
      "blocks.4.attn.b_Q\n",
      "blocks.4.attn.b_K\n",
      "blocks.4.attn.b_V\n",
      "blocks.4.attn.b_O\n",
      "blocks.4.mlp.W_in\n",
      "blocks.4.mlp.b_in\n",
      "blocks.4.mlp.W_out\n",
      "blocks.4.mlp.b_out\n",
      "blocks.5.ln1.w\n",
      "blocks.5.ln1.b\n",
      "blocks.5.ln2.w\n",
      "blocks.5.ln2.b\n",
      "blocks.5.attn.W_Q\n",
      "blocks.5.attn.W_K\n",
      "blocks.5.attn.W_V\n",
      "blocks.5.attn.W_O\n",
      "blocks.5.attn.b_Q\n",
      "blocks.5.attn.b_K\n",
      "blocks.5.attn.b_V\n",
      "blocks.5.attn.b_O\n",
      "blocks.5.mlp.W_in\n",
      "blocks.5.mlp.b_in\n",
      "blocks.5.mlp.W_out\n",
      "blocks.5.mlp.b_out\n",
      "blocks.6.ln1.w\n",
      "blocks.6.ln1.b\n",
      "blocks.6.ln2.w\n",
      "blocks.6.ln2.b\n",
      "blocks.6.attn.W_Q\n",
      "blocks.6.attn.W_K\n",
      "blocks.6.attn.W_V\n",
      "blocks.6.attn.W_O\n",
      "blocks.6.attn.b_Q\n",
      "blocks.6.attn.b_K\n",
      "blocks.6.attn.b_V\n",
      "blocks.6.attn.b_O\n",
      "blocks.6.mlp.W_in\n",
      "blocks.6.mlp.b_in\n",
      "blocks.6.mlp.W_out\n",
      "blocks.6.mlp.b_out\n",
      "blocks.7.ln1.w\n",
      "blocks.7.ln1.b\n",
      "blocks.7.ln2.w\n",
      "blocks.7.ln2.b\n",
      "blocks.7.attn.W_Q\n",
      "blocks.7.attn.W_K\n",
      "blocks.7.attn.W_V\n",
      "blocks.7.attn.W_O\n",
      "blocks.7.attn.b_Q\n",
      "blocks.7.attn.b_K\n",
      "blocks.7.attn.b_V\n",
      "blocks.7.attn.b_O\n",
      "blocks.7.mlp.W_in\n",
      "blocks.7.mlp.b_in\n",
      "blocks.7.mlp.W_out\n",
      "blocks.7.mlp.b_out\n",
      "blocks.8.ln1.w\n",
      "blocks.8.ln1.b\n",
      "blocks.8.ln2.w\n",
      "blocks.8.ln2.b\n",
      "blocks.8.attn.W_Q\n",
      "blocks.8.attn.W_K\n",
      "blocks.8.attn.W_V\n",
      "blocks.8.attn.W_O\n",
      "blocks.8.attn.b_Q\n",
      "blocks.8.attn.b_K\n",
      "blocks.8.attn.b_V\n",
      "blocks.8.attn.b_O\n",
      "blocks.8.mlp.W_in\n",
      "blocks.8.mlp.b_in\n",
      "blocks.8.mlp.W_out\n",
      "blocks.8.mlp.b_out\n",
      "blocks.9.ln1.w\n",
      "blocks.9.ln1.b\n",
      "blocks.9.ln2.w\n",
      "blocks.9.ln2.b\n",
      "blocks.9.attn.W_Q\n",
      "blocks.9.attn.W_K\n",
      "blocks.9.attn.W_V\n",
      "blocks.9.attn.W_O\n",
      "blocks.9.attn.b_Q\n",
      "blocks.9.attn.b_K\n",
      "blocks.9.attn.b_V\n",
      "blocks.9.attn.b_O\n",
      "blocks.9.mlp.W_in\n",
      "blocks.9.mlp.b_in\n",
      "blocks.9.mlp.W_out\n",
      "blocks.9.mlp.b_out\n",
      "blocks.10.ln1.w\n",
      "blocks.10.ln1.b\n",
      "blocks.10.ln2.w\n",
      "blocks.10.ln2.b\n",
      "blocks.10.attn.W_Q\n",
      "blocks.10.attn.W_K\n",
      "blocks.10.attn.W_V\n",
      "blocks.10.attn.W_O\n",
      "blocks.10.attn.b_Q\n",
      "blocks.10.attn.b_K\n",
      "blocks.10.attn.b_V\n",
      "blocks.10.attn.b_O\n",
      "blocks.10.mlp.W_in\n",
      "blocks.10.mlp.b_in\n",
      "blocks.10.mlp.W_out\n",
      "blocks.10.mlp.b_out\n",
      "blocks.11.ln1.w\n",
      "blocks.11.ln1.b\n",
      "blocks.11.ln2.w\n",
      "blocks.11.ln2.b\n",
      "blocks.11.attn.W_Q\n",
      "blocks.11.attn.W_K\n",
      "blocks.11.attn.W_V\n",
      "blocks.11.attn.W_O\n",
      "blocks.11.attn.b_Q\n",
      "blocks.11.attn.b_K\n",
      "blocks.11.attn.b_V\n",
      "blocks.11.attn.b_O\n",
      "blocks.11.mlp.W_in\n",
      "blocks.11.mlp.b_in\n",
      "blocks.11.mlp.W_out\n",
      "blocks.11.mlp.b_out\n",
      "ln_final.w\n",
      "ln_final.b\n",
      "head.W_H\n",
      "head.b_H\n"
     ]
    }
   ],
   "source": [
    "for k, v in hooked_state_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e6a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
