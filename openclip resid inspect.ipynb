{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b96a1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "og_model_name = 'laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K'\n",
    "model_name = 'hf-hub:' + og_model_name\n",
    "og_model, _, preproc = open_clip.create_model_and_transforms(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e762847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedViT(\n",
       "  (embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  )\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbedding()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (hook_full_embed): HookPoint()\n",
       "  (ln_pre): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_pre): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_final): HookPoint()\n",
       "  (head): Head()\n",
       "  (hook_post_head_pre_normalize): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "hooked_model = HookedViT.from_pretrained('open-clip:' + og_model_name, is_timm=False, is_clip=True, fold_ln=False, center_writing_weights=False) # in future, do all models\n",
    "hooked_model.to(\"cuda\")\n",
    "hooked_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2735822",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_hooked, cache = hooked_model.run_with_cache(image.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cab0fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[\"hook_embed\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "560197b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed\n",
      "hook_pos_embed\n",
      "hook_full_embed\n",
      "ln_pre.hook_scale\n",
      "ln_pre.hook_normalized\n",
      "hook_ln_pre\n",
      "blocks.0.hook_resid_pre\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.hook_resid_post\n",
      "blocks.1.hook_resid_pre\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.hook_resid_post\n",
      "blocks.2.hook_resid_pre\n",
      "blocks.2.ln1.hook_scale\n",
      "blocks.2.ln1.hook_normalized\n",
      "blocks.2.attn.hook_q\n",
      "blocks.2.attn.hook_k\n",
      "blocks.2.attn.hook_v\n",
      "blocks.2.attn.hook_attn_scores\n",
      "blocks.2.attn.hook_pattern\n",
      "blocks.2.attn.hook_z\n",
      "blocks.2.hook_attn_out\n",
      "blocks.2.hook_resid_mid\n",
      "blocks.2.ln2.hook_scale\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.2.mlp.hook_pre\n",
      "blocks.2.mlp.hook_post\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.2.hook_resid_post\n",
      "blocks.3.hook_resid_pre\n",
      "blocks.3.ln1.hook_scale\n",
      "blocks.3.ln1.hook_normalized\n",
      "blocks.3.attn.hook_q\n",
      "blocks.3.attn.hook_k\n",
      "blocks.3.attn.hook_v\n",
      "blocks.3.attn.hook_attn_scores\n",
      "blocks.3.attn.hook_pattern\n",
      "blocks.3.attn.hook_z\n",
      "blocks.3.hook_attn_out\n",
      "blocks.3.hook_resid_mid\n",
      "blocks.3.ln2.hook_scale\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.3.mlp.hook_pre\n",
      "blocks.3.mlp.hook_post\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.3.hook_resid_post\n",
      "blocks.4.hook_resid_pre\n",
      "blocks.4.ln1.hook_scale\n",
      "blocks.4.ln1.hook_normalized\n",
      "blocks.4.attn.hook_q\n",
      "blocks.4.attn.hook_k\n",
      "blocks.4.attn.hook_v\n",
      "blocks.4.attn.hook_attn_scores\n",
      "blocks.4.attn.hook_pattern\n",
      "blocks.4.attn.hook_z\n",
      "blocks.4.hook_attn_out\n",
      "blocks.4.hook_resid_mid\n",
      "blocks.4.ln2.hook_scale\n",
      "blocks.4.ln2.hook_normalized\n",
      "blocks.4.mlp.hook_pre\n",
      "blocks.4.mlp.hook_post\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.4.hook_resid_post\n",
      "blocks.5.hook_resid_pre\n",
      "blocks.5.ln1.hook_scale\n",
      "blocks.5.ln1.hook_normalized\n",
      "blocks.5.attn.hook_q\n",
      "blocks.5.attn.hook_k\n",
      "blocks.5.attn.hook_v\n",
      "blocks.5.attn.hook_attn_scores\n",
      "blocks.5.attn.hook_pattern\n",
      "blocks.5.attn.hook_z\n",
      "blocks.5.hook_attn_out\n",
      "blocks.5.hook_resid_mid\n",
      "blocks.5.ln2.hook_scale\n",
      "blocks.5.ln2.hook_normalized\n",
      "blocks.5.mlp.hook_pre\n",
      "blocks.5.mlp.hook_post\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.5.hook_resid_post\n",
      "blocks.6.hook_resid_pre\n",
      "blocks.6.ln1.hook_scale\n",
      "blocks.6.ln1.hook_normalized\n",
      "blocks.6.attn.hook_q\n",
      "blocks.6.attn.hook_k\n",
      "blocks.6.attn.hook_v\n",
      "blocks.6.attn.hook_attn_scores\n",
      "blocks.6.attn.hook_pattern\n",
      "blocks.6.attn.hook_z\n",
      "blocks.6.hook_attn_out\n",
      "blocks.6.hook_resid_mid\n",
      "blocks.6.ln2.hook_scale\n",
      "blocks.6.ln2.hook_normalized\n",
      "blocks.6.mlp.hook_pre\n",
      "blocks.6.mlp.hook_post\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.6.hook_resid_post\n",
      "blocks.7.hook_resid_pre\n",
      "blocks.7.ln1.hook_scale\n",
      "blocks.7.ln1.hook_normalized\n",
      "blocks.7.attn.hook_q\n",
      "blocks.7.attn.hook_k\n",
      "blocks.7.attn.hook_v\n",
      "blocks.7.attn.hook_attn_scores\n",
      "blocks.7.attn.hook_pattern\n",
      "blocks.7.attn.hook_z\n",
      "blocks.7.hook_attn_out\n",
      "blocks.7.hook_resid_mid\n",
      "blocks.7.ln2.hook_scale\n",
      "blocks.7.ln2.hook_normalized\n",
      "blocks.7.mlp.hook_pre\n",
      "blocks.7.mlp.hook_post\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.7.hook_resid_post\n",
      "blocks.8.hook_resid_pre\n",
      "blocks.8.ln1.hook_scale\n",
      "blocks.8.ln1.hook_normalized\n",
      "blocks.8.attn.hook_q\n",
      "blocks.8.attn.hook_k\n",
      "blocks.8.attn.hook_v\n",
      "blocks.8.attn.hook_attn_scores\n",
      "blocks.8.attn.hook_pattern\n",
      "blocks.8.attn.hook_z\n",
      "blocks.8.hook_attn_out\n",
      "blocks.8.hook_resid_mid\n",
      "blocks.8.ln2.hook_scale\n",
      "blocks.8.ln2.hook_normalized\n",
      "blocks.8.mlp.hook_pre\n",
      "blocks.8.mlp.hook_post\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.8.hook_resid_post\n",
      "blocks.9.hook_resid_pre\n",
      "blocks.9.ln1.hook_scale\n",
      "blocks.9.ln1.hook_normalized\n",
      "blocks.9.attn.hook_q\n",
      "blocks.9.attn.hook_k\n",
      "blocks.9.attn.hook_v\n",
      "blocks.9.attn.hook_attn_scores\n",
      "blocks.9.attn.hook_pattern\n",
      "blocks.9.attn.hook_z\n",
      "blocks.9.hook_attn_out\n",
      "blocks.9.hook_resid_mid\n",
      "blocks.9.ln2.hook_scale\n",
      "blocks.9.ln2.hook_normalized\n",
      "blocks.9.mlp.hook_pre\n",
      "blocks.9.mlp.hook_post\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.9.hook_resid_post\n",
      "blocks.10.hook_resid_pre\n",
      "blocks.10.ln1.hook_scale\n",
      "blocks.10.ln1.hook_normalized\n",
      "blocks.10.attn.hook_q\n",
      "blocks.10.attn.hook_k\n",
      "blocks.10.attn.hook_v\n",
      "blocks.10.attn.hook_attn_scores\n",
      "blocks.10.attn.hook_pattern\n",
      "blocks.10.attn.hook_z\n",
      "blocks.10.hook_attn_out\n",
      "blocks.10.hook_resid_mid\n",
      "blocks.10.ln2.hook_scale\n",
      "blocks.10.ln2.hook_normalized\n",
      "blocks.10.mlp.hook_pre\n",
      "blocks.10.mlp.hook_post\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.10.hook_resid_post\n",
      "blocks.11.hook_resid_pre\n",
      "blocks.11.ln1.hook_scale\n",
      "blocks.11.ln1.hook_normalized\n",
      "blocks.11.attn.hook_q\n",
      "blocks.11.attn.hook_k\n",
      "blocks.11.attn.hook_v\n",
      "blocks.11.attn.hook_attn_scores\n",
      "blocks.11.attn.hook_pattern\n",
      "blocks.11.attn.hook_z\n",
      "blocks.11.hook_attn_out\n",
      "blocks.11.hook_resid_mid\n",
      "blocks.11.ln2.hook_scale\n",
      "blocks.11.ln2.hook_normalized\n",
      "blocks.11.mlp.hook_pre\n",
      "blocks.11.mlp.hook_post\n",
      "blocks.11.hook_mlp_out\n",
      "blocks.11.hook_resid_post\n",
      "ln_final.hook_scale\n",
      "ln_final.hook_normalized\n",
      "hook_ln_final\n",
      "hook_post_head_pre_normalize\n"
     ]
    }
   ],
   "source": [
    "for key in cache:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e0ac98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6122645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[4.1990e-01, 5.7993e-01, 4.0174e-05, 1.3253e-04]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "image = preproc(Image.open(\"/workspace/ViT-Prisma/CLIP.png.1\")).unsqueeze(0)\n",
    "text = tokenizer([\"a chart\", \"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = og_model.encode_image(image)\n",
    "    text_features = og_model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f89eef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4814e-02,  5.6827e-04, -5.0106e-03,  2.5327e-02,  1.1405e-03,\n",
       "         -5.0535e-02,  2.6294e-02, -3.0264e-02, -2.0377e-02,  4.4621e-02,\n",
       "         -4.3194e-02, -9.8643e-02, -2.0076e-02,  2.1366e-03, -2.9839e-02,\n",
       "          9.7225e-03,  1.5830e-02, -2.2068e-02,  2.1483e-02, -1.9164e-02,\n",
       "         -7.6312e-04,  1.0234e-02,  1.5637e-02,  2.7757e-02,  6.0332e-03,\n",
       "          1.5497e-02, -9.6189e-03,  3.9546e-02, -1.5755e-02, -5.9842e-03,\n",
       "         -1.4172e-02,  1.1185e-02,  2.5468e-02, -9.9765e-02, -1.1933e-02,\n",
       "         -2.3282e-04,  2.0405e-02,  1.2976e-02, -8.7116e-03,  1.4091e-02,\n",
       "         -1.3447e-01, -3.8007e-02, -2.7824e-02, -9.2259e-03, -1.0193e-02,\n",
       "         -2.1133e-03, -2.1099e-02,  1.3982e-02, -5.8950e-02, -4.9354e-02,\n",
       "         -3.5513e-02,  4.2942e-02,  1.1141e-03, -2.2232e-02, -7.1256e-03,\n",
       "         -5.5039e-03,  1.1019e-01,  7.0468e-02, -1.3597e-02,  4.4308e-03,\n",
       "          9.5338e-03,  8.4288e-03, -2.7106e-03, -1.1791e-02,  3.5585e-02,\n",
       "         -4.9919e-02, -5.5466e-02, -7.0624e-03, -5.2167e-02, -3.0666e-02,\n",
       "          8.0005e-03, -4.1925e-02, -5.7697e-02, -2.8253e-02, -8.6503e-02,\n",
       "         -6.1902e-02, -4.1773e-02,  1.4792e-01, -7.4832e-03,  1.9859e-02,\n",
       "          6.3590e-03,  3.8533e-02,  1.1759e-02, -3.0358e-02, -2.5840e-02,\n",
       "          3.7633e-02,  1.7891e-02, -9.7606e-02,  3.0915e-02,  4.5219e-02,\n",
       "          2.2584e-02, -7.6674e-03, -1.4383e-02,  2.1193e-02,  6.3026e-03,\n",
       "         -4.8444e-02, -1.9842e-02,  4.4259e-03, -5.1849e-02,  2.8978e-02,\n",
       "          3.0253e-02, -3.3552e-03, -3.9892e-02, -2.7353e-04, -2.6058e-02,\n",
       "          4.1029e-02, -7.7709e-03, -2.9685e-02, -6.9294e-02, -1.7892e-02,\n",
       "          9.4022e-03, -1.3003e-02,  3.5302e-02, -8.8315e-02,  3.0991e-02,\n",
       "          1.8586e-02, -1.7661e-02,  2.9350e-02,  2.8300e-02,  4.3186e-02,\n",
       "         -2.4110e-02, -2.4964e-02,  1.8299e-02, -2.7340e-02,  5.6924e-02,\n",
       "         -3.5229e-02, -8.1064e-03, -2.1172e-02,  4.9810e-02,  5.4623e-02,\n",
       "          4.2316e-02, -1.6905e-02,  4.4860e-02,  4.9015e-03,  1.8559e-02,\n",
       "         -1.0329e-02, -3.2859e-02,  4.8080e-02,  1.8977e-02,  2.5483e-02,\n",
       "         -5.0016e-03, -4.5580e-02,  2.4828e-02,  7.8227e-03,  8.0928e-03,\n",
       "         -1.4732e-02,  2.2653e-02,  4.8149e-03,  3.4856e-02, -3.2340e-02,\n",
       "         -3.2707e-02,  3.4910e-02, -3.6462e-02,  1.9957e-02,  4.1341e-02,\n",
       "         -1.2040e-02,  2.2205e-02, -5.5094e-02, -1.7583e-02, -2.2798e-02,\n",
       "         -4.8626e-04,  8.9030e-04,  1.5218e-02,  5.0326e-02,  2.4051e-02,\n",
       "         -1.1242e-02,  4.2627e-02,  2.3991e-03, -1.9075e-02, -1.3249e-02,\n",
       "          2.3456e-02,  3.2326e-02, -5.3701e-02,  2.1649e-02,  3.7796e-02,\n",
       "         -3.0330e-02, -1.2350e-02, -5.3895e-03, -4.2400e-02,  1.7209e-02,\n",
       "         -4.0638e-02, -3.9228e-02, -2.1031e-02, -3.6671e-03,  3.5942e-02,\n",
       "          2.1925e-02,  7.0816e-03, -1.5114e-02, -1.6660e-02,  2.1225e-02,\n",
       "          2.1199e-02,  1.7169e-02, -1.7749e-02,  2.8377e-02, -1.6936e-02,\n",
       "          3.2786e-02, -2.8297e-02,  3.5679e-02,  1.4430e-02,  1.3871e-02,\n",
       "         -2.1346e-03,  2.1505e-02,  2.1043e-02, -2.5808e-02, -2.1875e-02,\n",
       "          2.0627e-02, -3.8005e-02,  3.4562e-02, -3.0976e-04, -2.3318e-02,\n",
       "          4.0927e-02, -4.3352e-02, -7.1860e-02, -1.2450e-03, -1.6140e-02,\n",
       "         -2.0438e-02,  1.0863e-01,  1.4633e-02, -2.5812e-02, -6.1365e-02,\n",
       "         -6.1997e-02,  9.9899e-02,  1.0290e-02, -3.7788e-02,  1.3266e-01,\n",
       "         -3.7797e-03,  1.6169e-02,  4.8392e-03, -5.0077e-03,  6.0582e-02,\n",
       "          4.5330e-02,  7.8242e-03, -2.9503e-02,  6.6242e-03,  5.1570e-02,\n",
       "         -1.4695e-02, -7.1305e-04,  3.6234e-02, -9.0090e-03,  1.7781e-02,\n",
       "         -9.2854e-03, -5.3416e-03, -1.4426e-02, -4.7597e-02,  2.6545e-02,\n",
       "         -2.7543e-02, -2.3316e-03, -1.2514e-02, -7.0453e-02,  2.7913e-02,\n",
       "         -8.0291e-03,  2.7989e-02, -1.1345e-01,  1.2113e-02, -1.0638e-02,\n",
       "          3.5976e-02, -4.1563e-03, -9.8319e-03,  3.3276e-02, -1.8536e-03,\n",
       "          2.7894e-02,  4.5301e-02,  4.2541e-02, -4.7688e-02,  8.0542e-02,\n",
       "          1.2272e-02,  1.2587e-02, -3.3819e-02, -2.0588e-02, -2.8274e-02,\n",
       "          1.2213e-02, -2.5837e-02,  3.4769e-02, -1.1439e-02,  6.8737e-02,\n",
       "         -9.8852e-03,  9.9669e-04,  4.1244e-02,  6.4701e-02, -1.9580e-02,\n",
       "          2.1130e-03,  4.1821e-02, -3.7338e-02, -4.2050e-02, -4.3649e-03,\n",
       "          1.4211e-02,  1.7581e-03,  7.5382e-03, -1.4681e-02, -2.8293e-02,\n",
       "          3.9795e-02,  4.9101e-03, -7.9438e-03,  6.1886e-02,  5.7245e-02,\n",
       "          1.0930e-02, -3.8034e-02,  1.9999e-02, -5.9782e-02, -1.2256e-02,\n",
       "         -6.1191e-03, -3.0302e-02, -2.1960e-02, -2.7312e-02,  8.7814e-03,\n",
       "         -2.6968e-02, -3.1678e-02, -6.6650e-03, -4.0774e-02, -3.9938e-02,\n",
       "          8.8312e-03,  4.4714e-02,  9.4761e-03,  2.1846e-02, -6.9809e-02,\n",
       "         -1.7418e-02,  6.7517e-03,  2.9628e-02,  1.3910e-03, -3.7512e-03,\n",
       "         -9.3080e-03, -5.5635e-02, -8.9648e-03,  4.5581e-02, -2.9501e-02,\n",
       "         -2.2523e-02, -6.1388e-02, -1.3872e-03, -7.7080e-04, -2.8563e-02,\n",
       "          8.5797e-03,  3.3658e-02, -5.6574e-03, -1.1144e-02, -4.1844e-02,\n",
       "          1.8568e-02,  4.7052e-02, -1.5774e-02,  1.5535e-02, -2.1270e-02,\n",
       "         -3.5557e-02,  3.1323e-02, -3.0727e-02, -6.3166e-02, -1.2358e-02,\n",
       "         -8.9316e-03,  1.9589e-02,  3.9378e-01, -4.6164e-02,  6.7164e-03,\n",
       "          4.3794e-02,  3.2699e-02, -2.2910e-02, -4.4239e-02, -1.9436e-02,\n",
       "         -1.9450e-03,  1.1931e-02, -2.4649e-02,  1.1101e-03,  7.1902e-02,\n",
       "          2.8727e-02, -1.3536e-02,  3.5220e-02,  2.7733e-02, -2.7427e-02,\n",
       "         -2.3825e-02, -6.0817e-02, -6.0093e-02,  3.5406e-02, -2.9394e-02,\n",
       "         -1.4164e-02, -2.5408e-02,  3.4998e-02, -3.0223e-02,  3.6669e-02,\n",
       "         -7.4470e-03, -1.3198e-02,  1.2985e-02, -1.8196e-02, -2.6317e-02,\n",
       "         -2.9886e-02, -4.2008e-02, -5.0504e-02, -3.3601e-03, -1.4345e-02,\n",
       "         -1.6980e-02, -3.1127e-02,  9.1892e-03,  8.6671e-03,  3.4073e-03,\n",
       "          4.7307e-02, -4.2335e-02, -1.9626e-02, -3.3536e-02,  2.1778e-02,\n",
       "          1.5605e-02,  7.5821e-02,  2.9850e-02, -3.3670e-02,  6.0776e-02,\n",
       "         -1.5728e-02,  4.1743e-02, -1.3415e-02, -1.4699e-02, -2.6175e-02,\n",
       "         -1.3511e-02,  5.1348e-03,  2.5377e-02,  1.9593e-02,  5.9887e-02,\n",
       "          3.5611e-02,  3.5872e-02, -5.5401e-02, -2.9201e-02, -1.7898e-02,\n",
       "         -7.7823e-02,  1.5996e-02,  2.1209e-02, -9.4789e-03,  2.9932e-04,\n",
       "          2.1061e-03,  1.5615e-02,  1.2631e-02,  4.1368e-03,  2.1653e-03,\n",
       "          2.6701e-02,  6.7640e-03,  2.4053e-02,  4.4938e-03, -4.5552e-02,\n",
       "          4.5694e-02,  2.1590e-02, -1.4535e-02,  8.5863e-03, -1.3816e-01,\n",
       "          5.9865e-02,  2.3775e-02, -2.4064e-02, -1.8305e-02,  1.7783e-02,\n",
       "          1.2772e-02, -7.4681e-02, -2.4827e-02, -1.8645e-02,  2.9157e-02,\n",
       "          5.6885e-03,  3.5156e-02,  2.8955e-03,  2.5785e-04,  2.2987e-02,\n",
       "         -4.6870e-02,  7.6254e-02, -2.6467e-02, -2.8932e-02,  3.9842e-01,\n",
       "          1.5078e-02,  9.9969e-03, -8.4576e-02, -1.5559e-02,  2.7194e-02,\n",
       "         -4.4139e-03,  6.4551e-02, -8.1819e-03, -6.2363e-02, -1.5554e-02,\n",
       "          2.4914e-02,  7.1950e-02, -5.6154e-03, -4.1067e-02,  1.1396e-02,\n",
       "         -4.1087e-02, -2.6647e-02, -5.0652e-02,  4.7110e-02,  3.1771e-02,\n",
       "          3.8765e-02,  1.1349e-02,  1.9301e-02,  9.7997e-02,  1.7834e-02,\n",
       "          6.0918e-03, -5.9289e-03, -4.5279e-03, -4.5822e-02, -4.8232e-02,\n",
       "          7.9493e-03, -3.5955e-02,  2.0741e-02, -2.3483e-03,  1.7211e-04,\n",
       "          1.6058e-01,  1.6984e-02, -2.9132e-02,  4.7099e-02, -7.0739e-02,\n",
       "         -6.7440e-04, -3.2682e-03, -2.2323e-02, -4.7869e-02, -2.3578e-02,\n",
       "         -6.2426e-02, -2.1417e-02, -2.8781e-02, -6.4402e-02, -2.9287e-02,\n",
       "         -4.3081e-02, -5.5499e-02,  1.2016e-02,  2.1503e-02,  5.3038e-02,\n",
       "          3.9192e-02,  2.0528e-02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deebf849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0135, -0.0410,  0.0104,  ..., -0.0148,  0.0418,  0.0083],\n",
       "        [ 0.0047, -0.0260, -0.0183,  ..., -0.0194,  0.0283, -0.0253],\n",
       "        [-0.0332, -0.0373, -0.0535,  ..., -0.0360,  0.0324,  0.0046],\n",
       "        [-0.0196, -0.0310, -0.0072,  ..., -0.0623, -0.0179,  0.0553]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13576737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
