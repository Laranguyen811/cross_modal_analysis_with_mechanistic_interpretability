{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from vit_prisma.utils.constants import BASE_DIR, DATA_DIR, MODEL_DIR, DEVICE, MODEL_CHECKPOINTS_DIR\n",
    "from vit_prisma.utils.tutorial_utils import calculate_clean_accuracy, load_clip_models, plot_image, get_feature_activations\n",
    "from vit_prisma.utils.data_utils.loader import load_dataset\n",
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "from vit_prisma.sae.train_sae import VisionSAETrainer\n",
    "from vit_prisma.utils.tutorial_utils import plot_act_distribution\n",
    "from vit_prisma.utils.tutorial_utils import plot_top_imgs_for_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prisma SAE Tutorial\n",
    "\n",
    "This tutorial is a quick start guide on how to load, evaluate, and train SAEs with Prisma. It uses ImageNet as an extended example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Ensure you have ImageNet data locally (you will need to have a Kaggle account).\n",
    "\n",
    "```\n",
    "python3 -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install -e .\n",
    "pip install kaggle\n",
    "cd data\n",
    "kaggle competitions download -c imagenet-object-localization-challenge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # change to cpu if cpu only paradigm\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained SAE\n",
    "\n",
    "Prisma has trained several SAEs for different architectures that can be found on HuggingFace [here](https://huggingface.co/Prisma-Multimodal). Here we will use a CLIP SAE trained on ImageNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an SAE\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "\n",
    "def load_sae(repo_id, file_name, config_name):\n",
    "    # Step 1: Download SAE from Hugginface\n",
    "    sae_path = hf_hub_download(repo_id, file_name) # Download weights\n",
    "    hf_hub_download(repo_id, config_name) # Download config\n",
    "\n",
    "    # Step 2: Load the pretrained SAE weights from the downloaded path\n",
    "    print(f\"Loading SAE from {sae_path}...\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path) # This now automatically gets config.json and converts into the VisionSAERunnerConfig object\n",
    "    return sae\n",
    "\n",
    "repo_id = \"Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05\" # Change this to your chosen SAE. See /docs for a list of SAEs.\n",
    "file_name = \"weights.pt\" # Usually weights.pt but may have slight naming variation. See the original HF repo for the exact file name\n",
    "config_name = \"config.json\"\n",
    "sae = load_sae(repo_id, file_name, config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sae's config and examine some details. In particular, we confirm that it was trained on the layer 11 residual stream of the base model `open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "sae.cfg\n",
    "pprint(sae.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model\n",
    "\n",
    "Let's load the pretrained CLIP model using the `load_hooked_model` function. The function loads the pretrained CLIP weights from Huggingface and puts the weights into our HookedViT object so that we can readily grab the intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "\n",
    "model_name = sae.cfg.model_name\n",
    "model = load_hooked_model(model_name)\n",
    "model.to(DEVICE) # Move to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put paths here\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "import torchvision\n",
    "\n",
    "def load_imagenet(imagenet_validation_path):\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "    # Load dataset with CLIP transform\n",
    "    data_transforms = get_clip_val_transforms()\n",
    "    val_data = torchvision.datasets.ImageFolder(imagenet_validation_path, transform=data_transforms)\n",
    "\n",
    "    # We'll also load a version of the dataset without the CLIP transform so that we can visualize it beautifully\n",
    "    viz_transforms = torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.Resize((224, 224)),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                    ]\n",
    "                )\n",
    "    viz_data = torchvision.datasets.ImageFolder(imagenet_validation_path, transform=viz_transforms)\n",
    "\n",
    "    # We only want a subset of validation\n",
    "            \n",
    "    subset_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # We only want a subset \n",
    "    \n",
    "    return subset_dataloader, val_data, viz_data\n",
    "\n",
    "# Put your imagenet path here. You can download ImageNet from kaggle.\n",
    "imagenet_validation_path = '/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val'\n",
    "subset_dataloader, subset_dataset, viz_data = load_imagenet(imagenet_validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP can work as a zero-shot classifier. Because it learns a joint embedding space for both images and text during pre-training, it can match new images with text descriptions of the classes without requiring any examples of the target classes during inference. Here we load the text embeddings of the class names from file.\n",
    "\n",
    "So let's load the CLIP classifier for ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_npy_path = '../pretrained_classifiers/clip_benchmark/imagenet_classifier_hf_hub_laion_CLIP_ViT_B_32_DataComp.XL_s13B_b90K.npy'\n",
    "classifier_class_vectors = np.load(classifier_npy_path)\n",
    "classifier_class_vectors = torch.from_numpy(classifier_class_vectors).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calculate_clean_accuracy(\n",
    "    net,\n",
    "    classifier: torch.Tensor,\n",
    "    data_loader,\n",
    "    device=DEVICE,\n",
    "    top_k: int = 1,\n",
    "    sae: SparseAutoencoder = None,\n",
    "):\n",
    "    \"\"\"Calculate the top k clean accuracy of a CLIPmodel on a dataset.\"\"\"\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = sae.get_test_loss(images, net) if sae else net(images)\n",
    "            logits = 100.0 * logits @ classifier\n",
    "            preds = logits.topk(top_k)[1].t()[0]\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "            if total > 100: # We'll just check the accuracy on 100 images for brevity.\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the accuracy of CLIP on 50 ImageNet images just as a sanity check! It should be >69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc, total = calculate_clean_accuracy(model, classifier_class_vectors, subset_dataloader)\n",
    "print(f\"Accuracy is {acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load a particular image to put through the SAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "def plot_image(image, unstandardise=True):\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if unstandardise:\n",
    "        print(\"Unstandardizing image\")\n",
    "        get_inverse = transforms.Normalize(\n",
    "            mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "            std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    "        )\n",
    "        image = get_inverse(image)\n",
    "\n",
    "    image = image.permute(1, 2, 0)\n",
    "    plt.imshow(image, vmin=-2.5, vmax=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download both images using wget\n",
    "!wget -O normal_image.jpg https://raw.githubusercontent.com/Prisma-Multimodal/ViT-Prisma/dev/src/vit_prisma/sample_images/n01818515_39.JPEG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "\n",
    "img = Image.open(\"normal_image.jpg\").convert()  # Ensure it's 3 channels\n",
    "transforms = get_clip_val_transforms()\n",
    "img_tensor = transforms(img)\n",
    "plot_image(img_tensor.detach().cpu(), unstandardise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed the image into an SAE and look at the top-activating features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_activations(model_input, model, sae):\n",
    "    # Run the batch through the model to get activations\n",
    "    _, cache = model.run_with_cache(model_input, names_filter=sae.cfg.hook_point)\n",
    "    hook_point_activation = cache[sae.cfg.hook_point].to(DEVICE)\n",
    "\n",
    "    # Calculate the SAE features and stats for the batch\n",
    "    _, feature_acts, *_ = sae(hook_point_activation)\n",
    "\n",
    "    return feature_acts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature activations are batch x patches x dimensionality. We're just going to grab the first batch and the CLS token (index 0). We'll ignore the spatial patches for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_activations = get_feature_activations(img_tensor.to(DEVICE).unsqueeze(0), model, sae)\n",
    "print(feature_activations.shape)\n",
    "feature_activations = feature_activations[0,0,:]\n",
    "print(feature_activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices, _ = plot_act_distribution(\n",
    "    feature_activations,\n",
    "    n_top=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Set, Optional, Union\n",
    "import einops\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    is_cls_list: List[bool],\n",
    "    top_k: int = 10,\n",
    "    sampling_type: str = 'max'\n",
    ") -> Dict[int, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        is_cls_list: Whether to use CLS token activations for each feature\n",
    "        top_k: Number of top activations to return per feature\n",
    "        sampling_type: How to sample activations across tokens ('avg' for average)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "    # Run model and get activations\n",
    "    _, cache = model.run_with_cache(images, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "    \n",
    "    layer_activations = cache[sparse_autoencoder.cfg.hook_point]\n",
    "    batch_size, seq_len, _ = layer_activations.shape\n",
    "\n",
    "    # Ensure top_k is not greater than batch size\n",
    "    actual_top_k = min(top_k, batch_size)\n",
    "    \n",
    "    # Flatten activations for efficient computation\n",
    "    flattened_activations = einops.rearrange(layer_activations, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    \n",
    "    # Compute feature activations using the SAE\n",
    "    sae_input = flattened_activations - sparse_autoencoder.b_dec\n",
    "    feature_activations = einops.einsum(sae_input, encoder_weights, \"... d_in, d_in n -> ... n\") + encoder_biases\n",
    "    feature_activations = torch.nn.functional.relu(feature_activations)\n",
    "    \n",
    "    # Reshape back to batch × seq × features\n",
    "    reshaped_activations = einops.rearrange(\n",
    "        feature_activations, \n",
    "        \"(batch seq) features -> batch seq features\", \n",
    "        batch=batch_size, \n",
    "        seq=seq_len\n",
    "    )\n",
    "    \n",
    "    # Get CLS token activations (first token)\n",
    "    cls_token_activations = reshaped_activations[:, 0, :]\n",
    "    \n",
    "    # Get average activations across tokens if requested\n",
    "    if sampling_type == 'avg':\n",
    "        mean_image_activations = reshaped_activations.mean(dim=1)\n",
    "    elif sampling_type == 'max':\n",
    "        # Get maximum activation across all tokens for each feature\n",
    "        mean_image_activations, _ = reshaped_activations.max(dim=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid sampling type: {sampling_type}, or not implemented yet\")\n",
    "\n",
    "    # Create result dictionary\n",
    "    top_activations = {}\n",
    "    \n",
    "    # For each feature, get top-k activating images\n",
    "    for feature_idx, (feature_id, is_cls) in enumerate(zip(feature_ids, is_cls_list)):\n",
    "        if is_cls:\n",
    "            # Use CLS token activations\n",
    "            feature_values = cls_token_activations[:, feature_idx]\n",
    "        else:\n",
    "            # Use mean activations across tokens\n",
    "            feature_values = mean_image_activations[:, feature_idx]\n",
    "        \n",
    "        # Get top-k values and indices\n",
    "        top_values, top_indices = feature_values.topk(actual_top_k)\n",
    "        top_activations[feature_id] = (top_indices, top_values)\n",
    "    \n",
    "    return top_activations\n",
    "\n",
    "def find_top_activations(\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    interesting_features_indices: List[int],\n",
    "    is_cls_list: List[bool],\n",
    "    top_k: int = 16,\n",
    "    max_samples: int = 50_000,\n",
    "    batch_size: int = 64, \n",
    "    sampling_type: str = 'avg'\n",
    ") -> Dict[int, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Find the top activations for interesting features across the validation dataset.\n",
    "    \n",
    "    Args:\n",
    "        val_dataloader: DataLoader for validation dataset\n",
    "        model: Model to analyze\n",
    "        sparse_autoencoder: Sparse autoencoder\n",
    "        interesting_features_indices: List of feature indices to analyze\n",
    "        is_cls_list: Whether to use CLS token for each feature\n",
    "        top_k: Number of top images to return per feature\n",
    "        max_samples: Maximum number of samples to process\n",
    "        batch_size: Batch size for processing\n",
    "        sampling_type: How to sample activations across tokens\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_values, top_dataset_indices)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Dictionary to store values and indices for all features\n",
    "    all_feature_activations = {}\n",
    "    for feature_id in interesting_features_indices:\n",
    "        all_feature_activations[feature_id] = {\n",
    "            'values': [],\n",
    "            'dataset_indices': []\n",
    "        }\n",
    "    \n",
    "    # Extract weights and biases for relevant features\n",
    "    encoder_biases = sparse_autoencoder.b_enc[interesting_features_indices]\n",
    "    encoder_weights = sparse_autoencoder.W_enc[:, interesting_features_indices]\n",
    "\n",
    "    # Track global dataset indices\n",
    "    global_idx = 0\n",
    "    processed_samples = 0\n",
    "    \n",
    "    # Process batches\n",
    "    for batch_idx, (batch_images, batch_labels) in enumerate(tqdm(val_dataloader, total=min(len(val_dataloader), max_samples // batch_size))):\n",
    "        # Move batch to device\n",
    "        batch_images = batch_images.to(device)\n",
    "        current_batch_size = batch_images.shape[0]\n",
    "        \n",
    "        # Generate dataset indices for this batch\n",
    "        batch_dataset_indices = torch.arange(\n",
    "            global_idx, \n",
    "            global_idx + current_batch_size, \n",
    "            dtype=torch.long, \n",
    "            device=device\n",
    "        )\n",
    "        global_idx += current_batch_size\n",
    "        \n",
    "        # Get activations for this batch\n",
    "        batch_activations = compute_feature_activations(\n",
    "            batch_images, \n",
    "            model, \n",
    "            sparse_autoencoder, \n",
    "            encoder_weights, \n",
    "            encoder_biases,\n",
    "            interesting_features_indices, \n",
    "            is_cls_list, \n",
    "            top_k=top_k,\n",
    "            sampling_type=sampling_type\n",
    "        )\n",
    "\n",
    "        # For each feature, store values and corresponding dataset indices\n",
    "        for feature_id in interesting_features_indices:\n",
    "            # Get top indices and values for this feature in this batch\n",
    "            batch_top_indices, batch_top_values = batch_activations[feature_id]\n",
    "            \n",
    "            # Map batch indices to dataset indices\n",
    "            dataset_indices = batch_dataset_indices[batch_top_indices]\n",
    "            \n",
    "            # Store values and indices\n",
    "            all_feature_activations[feature_id]['values'].append(batch_top_values)\n",
    "            all_feature_activations[feature_id]['dataset_indices'].append(dataset_indices)\n",
    "\n",
    "        # Update processed samples count\n",
    "        processed_samples += current_batch_size\n",
    "        if processed_samples >= max_samples:\n",
    "            break\n",
    "\n",
    "    # Process all activations to get overall top-k\n",
    "    final_results = {}\n",
    "    \n",
    "    for feature_id in interesting_features_indices:\n",
    "        # Concatenate all values and indices for this feature\n",
    "        all_values = torch.cat(all_feature_activations[feature_id]['values']) if all_feature_activations[feature_id]['values'] else torch.tensor([], device=device)\n",
    "        all_indices = torch.cat(all_feature_activations[feature_id]['dataset_indices']) if all_feature_activations[feature_id]['dataset_indices'] else torch.tensor([], device=device, dtype=torch.long)\n",
    "        \n",
    "        # Handle empty case\n",
    "        if len(all_indices) == 0:\n",
    "            final_results[feature_id] = (torch.tensor([]), torch.tensor([]))\n",
    "            continue\n",
    "        \n",
    "        # Find unique dataset indices and keep the highest activation per index\n",
    "        unique_dict = {}\n",
    "        for i, (val, idx) in enumerate(zip(all_values, all_indices)):\n",
    "            idx_item = idx.item()\n",
    "            if idx_item not in unique_dict or val > all_values[unique_dict[idx_item]]:\n",
    "                unique_dict[idx_item] = i\n",
    "        \n",
    "        # Extract unique values and indices\n",
    "        unique_values = torch.tensor([all_values[i].item() for i in unique_dict.values()], device=device)\n",
    "        unique_dataset_indices = torch.tensor(list(unique_dict.keys()), device=device, dtype=torch.long)\n",
    "        \n",
    "        # Get top-k\n",
    "        if len(unique_values) > 0:\n",
    "            k = min(top_k, len(unique_values))\n",
    "            top_k_values, top_k_idxs = torch.topk(unique_values, k)\n",
    "            top_dataset_indices = unique_dataset_indices[top_k_idxs]\n",
    "            \n",
    "            final_results[feature_id] = (top_k_values.detach().cpu(), top_dataset_indices.detach().cpu())\n",
    "        else:\n",
    "            final_results[feature_id] = (torch.tensor([]), torch.tensor([]))\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_activations_dictionary = find_top_activations(subset_dataloader, model, sae, top_indices, [True]*len(top_indices), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_features(top_activations_dict, viz_data, cfg, num_features=None):\n",
    "    \"\"\"\n",
    "    Plot top activating images for all features in the dictionary.\n",
    "    \n",
    "    Args:\n",
    "        top_activations_dict: Dictionary mapping feature_idx to (activation_values, image_indices)\n",
    "        viz_data: Dataset containing images\n",
    "        cfg: Configuration object with hook_point\n",
    "        num_features: Optional limit on number of features to plot (None = all)\n",
    "    \"\"\"\n",
    "    # Sort features by their keys to ensure consistent order\n",
    "    feature_indices = sorted(top_activations_dict.keys())\n",
    "    \n",
    "    # Limit number of features if specified\n",
    "    if num_features is not None:\n",
    "        feature_indices = feature_indices[:num_features]\n",
    "    \n",
    "    print(f\"Plotting {len(feature_indices)} features\")\n",
    "    \n",
    "    # Plot each feature\n",
    "    for feature_idx in feature_indices:\n",
    "        activation_values, imagenet_indices = top_activations_dict[feature_idx]\n",
    "        \n",
    "        # Create plot grid\n",
    "        grid_size = int(np.ceil(np.sqrt(len(imagenet_indices))))\n",
    "        fig, axs = plt.subplots(int(np.ceil(len(imagenet_indices) / grid_size)), grid_size, figsize=(8, 8))\n",
    "        fig.suptitle(f\"Layer: {cfg.hook_point}, Feature: {feature_idx}\")\n",
    "        \n",
    "        axs = axs.flatten()\n",
    "        for i, (image_idx, act_value) in enumerate(zip(imagenet_indices, activation_values)):\n",
    "            # Get and display image\n",
    "            image_idx_int = image_idx.item()\n",
    "            image_tensor = viz_data[image_idx_int][0]  # Assuming viz_data returns (image, _, _)\n",
    "\n",
    "            \n",
    "            # Convert to numpy for display\n",
    "            display = image_tensor.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            mean = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))\n",
    "            std = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))\n",
    "            display = display * std + mean\n",
    "            \n",
    "            # Clip values to valid image range [0, 1]\n",
    "            display = np.clip(display, 0, 1)\n",
    "            \n",
    "            axs[i].imshow(display)\n",
    "            axs[i].set_title(f\"Img idx: {image_idx_int} Act: {act_value.item():.3f}\")\n",
    "            axs[i].axis(\"off\")\n",
    "        \n",
    "        # Turn off unused subplots\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            axs[j].axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_features(top_activations_dictionary, subset_dataset, sae.cfg, num_features=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a model - you can either load one or use Prisma to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from vit_prisma.training import trainer\n",
    "\n",
    "\n",
    "sae_config = VisionModelSAERunnerConfig(\n",
    "    num_epochs=10,\n",
    "    n_checkpoints=5,\n",
    "    log_to_wandb=False,\n",
    "    verbose=True,\n",
    "    wandb_log_frequency=1,\n",
    "    dataset_name=\"imagenet\",\n",
    "    dataset_path=str(DATA_DIR / \"imagenet\"),\n",
    "    train_batch_size=1024,\n",
    "    hook_point_layer=3,\n",
    "    device=DEVICE,\n",
    "    total_training_images=48000,\n",
    "    total_training_tokens=48000 * 65 * 10,\n",
    "    model_name=\"local/imagenet\",\n",
    "    sae_path=str(MODEL_DIR / \"sae/imagenet/checkpoints\"),\n",
    "    context_size=65,\n",
    "    image_size=128,\n",
    ")\n",
    "\n",
    "model_function = HookedViT\n",
    "trainer.train(\n",
    "    model_function,\n",
    "    sae_config,\n",
    "    train_dataset=train_data,\n",
    "    val_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the SAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_train_cfg = VisionModelSAERunnerConfig(\n",
    "    model_name=\"local/imagenet\",\n",
    "    hook_point_layer=3,\n",
    "    d_in=384,\n",
    "    context_size=65,\n",
    "    dataset_name=\"imagenet\",\n",
    "    dataset_path=str(DATA_DIR / \"imagenet\"),\n",
    "    dataset_train_path=str(DATA_DIR / \"imagenet/ILSVRC/Data/CLS-LOC/train\"),\n",
    "    dataset_val_path=str(DATA_DIR / \"imagenet/ILSVRC/Data/CLS-LOC/val\"),\n",
    "    feature_sampling_window=1000,\n",
    ")\n",
    "\n",
    "trainer = VisionSAETrainer(sae_train_cfg)\n",
    "sae = trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the SAE\n",
    "\n",
    "This notebook is still under development - check back soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model, test_data, cfg)\n",
    "evaluator.evaluate(sae, context=EvaluationContext.POST_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
