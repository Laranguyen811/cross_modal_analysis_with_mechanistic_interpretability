{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to timm-pretrained model, then turn into series of unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.configs import HookedViTConfig\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "import timm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf config ViTConfig {\n",
      "  \"_name_or_path\": \"timm/vit_base_patch32_224.augreg_in21k_ft_in1k\",\n",
      "  \"architecture\": \"vit_base_patch32_224\",\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"global_pool\": \"token\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_classes\": 1000,\n",
      "  \"num_features\": 768,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pretrained_cfg\": {\n",
      "    \"classifier\": \"head\",\n",
      "    \"crop_mode\": \"center\",\n",
      "    \"crop_pct\": 0.9,\n",
      "    \"custom_load\": true,\n",
      "    \"first_conv\": \"patch_embed.proj\",\n",
      "    \"fixed_input_size\": true,\n",
      "    \"input_size\": [\n",
      "      3,\n",
      "      224,\n",
      "      224\n",
      "    ],\n",
      "    \"interpolation\": \"bicubic\",\n",
      "    \"mean\": [\n",
      "      0.5,\n",
      "      0.5,\n",
      "      0.5\n",
      "    ],\n",
      "    \"num_classes\": 1000,\n",
      "    \"pool_size\": null,\n",
      "    \"std\": [\n",
      "      0.5,\n",
      "      0.5,\n",
      "      0.5\n",
      "    ],\n",
      "    \"tag\": \"augreg_in21k_ft_in1k\"\n",
      "  },\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.37.2\"\n",
      "}\n",
      "\n",
      "{'n_layers': 12, 'd_model': 768, 'd_head': 64, 'model_name': 'timm/vit_base_patch32_224.augreg_in21k_ft_in1k', 'n_heads': 12, 'd_mlp': 3072, 'activation_name': 'gelu', 'eps': 1e-12, 'original_architecture': 'vit_base_patch32_224', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 16, 'image_size': 224, 'n_classes': 1000, 'n_params': 88224232}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HookedViT:\n\tsize mismatch for embed.proj.weight: copying a param with shape torch.Size([768, 3, 32, 32]) from checkpoint, the shape in current model is torch.Size([768, 3, 16, 16]).\n\tsize mismatch for pos_embed.W_pos: copying a param with shape torch.Size([50, 768]) from checkpoint, the shape in current model is torch.Size([197, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prisma_model \u001b[38;5;241m=\u001b[39m \u001b[43mHookedViT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvit_base_patch32_224\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;43;03m#                                          center_writing_weights=False, \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;43;03m#                                          fold_ln=False, \u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;43;03m#                                          fold_value_biases=False,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;43;03m#                                          use_attn_scale=False,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;43;03m#                                          use_attn_in=True,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# timm_model = timm.create_model('vit_base_patch16_224', pretrained=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/ViT-Planetarium/src/vit_prisma/models/base_vit.py:671\u001b[0m, in \u001b[0;36mHookedViT.from_pretrained\u001b[0;34m(cls, model_name, is_timm, is_clip, fold_ln, center_writing_weights, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m get_pretrained_state_dict(\n\u001b[1;32m    666\u001b[0m     model_name, is_timm, cfg, hf_model, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs\n\u001b[1;32m    667\u001b[0m )\n\u001b[1;32m    669\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(cfg, move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 671\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_process_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[1;32m    680\u001b[0m     model\u001b[38;5;241m.\u001b[39mmove_model_modules_to_device()\n",
      "File \u001b[0;32m~/ViT-Planetarium/src/vit_prisma/models/base_vit.py:589\u001b[0m, in \u001b[0;36mHookedViT.load_and_process_state_dict\u001b[0;34m(self, state_dict, fold_ln, center_writing_weights, fold_value_biases, refactor_factored_attn_matrices)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refactor_factored_attn_matrices:\n\u001b[1;32m    587\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefactor_factored_attn_matrices(state_dict)\n\u001b[0;32m--> 589\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ViT-Planetarium/prisma_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HookedViT:\n\tsize mismatch for embed.proj.weight: copying a param with shape torch.Size([768, 3, 32, 32]) from checkpoint, the shape in current model is torch.Size([768, 3, 16, 16]).\n\tsize mismatch for pos_embed.W_pos: copying a param with shape torch.Size([50, 768]) from checkpoint, the shape in current model is torch.Size([197, 768])."
     ]
    }
   ],
   "source": [
    "prisma_model = HookedViT.from_pretrained(\"vit_base_patch32_224\", \n",
    "#                                          center_writing_weights=False, \n",
    "#                                          fold_ln=False, \n",
    "#                                          fold_value_biases=False,\n",
    "#                                          use_attn_scale=False,\n",
    "#                                          use_attn_in=True,\n",
    ")\n",
    "# timm_model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images to [-1, 1]\n",
    "    # Resize to 224 x 224\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(testloader))\n",
    "output, cache = prisma_model.run_with_cache(image)\n",
    "\n",
    "for key in cache.keys():\n",
    "    print(key, cache[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.patch_embed.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "activations[0].shape\n",
    "\n",
    "assert torch.allclose(activations[0], cache['embed'][0])\n",
    "assert torch.all(activations[0] == cache['embed'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.pos_drop.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "assert torch.allclose(activations[0], cache['blocks.0.hook_resid_pre'], atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import einops \n",
    "\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.blocks[0].norm1.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(activations[0].shape)\n",
    "print(cache['blocks.0.ln1.hook_normalized'].shape)\n",
    "\n",
    "# Assert equal to the first layer\n",
    "assert torch.allclose(activations[0], cache['blocks.0.ln1.hook_normalized'][0], atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "**Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's compare qkv weights\n",
    "# QKV = timm_model.blocks[0].attn.qkv.weight\n",
    "# W_Q, W_K, W_V = torch.tensor_split(QKV, 3, dim=0)\n",
    "# t_Q = einops.rearrange(W_Q, \"(i h) m->h m i\", h=12)\n",
    "# t_K = einops.rearrange(W_K, \"(i h) m->h m i\", h=12)\n",
    "# t_V = einops.rearrange(W_V, \"(i h) m->h m i\", h=12)\n",
    "\n",
    "# p_Q = prisma_model.blocks[0].attn.W_Q\n",
    "# p_K = prisma_model.blocks[0].attn.W_K\n",
    "# p_V = prisma_model.blocks[0].attn.W_V\n",
    "\n",
    "# assert torch.allclose(p_Q, t_Q, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(p_K, t_K, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(p_V, t_V, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "\n",
    "# # qkv bias\n",
    "# bias_QKV = timm_model.blocks[0].attn.qkv.bias\n",
    "\n",
    "# b_Q, b_K, b_V = torch.tensor_split(bias_QKV, 3, dim=0)\n",
    "\n",
    "# bt_Q = einops.rearrange(b_Q, \"(i h) -> h i\", h=12)\n",
    "# bt_K = einops.rearrange(b_K, \"(i h) -> h i\", h=12)\n",
    "# bt_V = einops.rearrange(b_V, \"(i h) -> h i\", h=12)\n",
    "\n",
    "# bp_Q = prisma_model.blocks[0].attn.b_Q\n",
    "# bp_K = prisma_model.blocks[0].attn.b_K\n",
    "# bp_V = prisma_model.blocks[0].attn.b_V\n",
    "\n",
    "# assert torch.allclose(bp_Q, bt_Q, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(bp_K, bt_K, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(bp_V, bt_V, atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix_corner(matrix, rows=1, cols=1):\n",
    "    \"\"\"\n",
    "    Prints the top-left corner of a matrix (tensor) up to the specified number of rows and columns.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix (torch.Tensor): The matrix (tensor) from which to print the corner.\n",
    "    - rows (int): The number of rows to include in the printed corner. Default is 5.\n",
    "    - cols (int): The number of columns to include in the printed corner. Default is 5.\n",
    "    \"\"\"\n",
    "    # Ensure the matrix is a PyTorch tensor\n",
    "    if not isinstance(matrix, torch.Tensor):\n",
    "        print(\"The input is not a PyTorch tensor.\")\n",
    "        return\n",
    "\n",
    "    # Get the size of the matrix\n",
    "    num_rows, num_cols = matrix.shape[:2]\n",
    "\n",
    "    # Adjust rows and cols if the matrix is smaller than specified dimensions\n",
    "    rows_to_print = min(rows, num_rows)\n",
    "    cols_to_print = min(cols, num_cols)\n",
    "\n",
    "    # Slice the matrix to get the top-left corner\n",
    "    corner = matrix[:rows_to_print, :cols_to_print]\n",
    "\n",
    "    print(f\"Top-left corner ({rows_to_print}x{cols_to_print}):\\n{corner}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QKV matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.blocks[0].attn.qkv.register_forward_hook(hook_fn)\n",
    "\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(\"timm output\", activations[0].shape)\n",
    "# qkv = activations[0].reshape(-1, 197, 3, 12, 64).permute(2, 0, 3, 1, 4)\n",
    "qkv = activations[0].reshape(-1, 197, 3, 12, 64).permute(2, 0, 3, 1, 4)\n",
    "q, k, v = qkv.unbind(0)\n",
    "\n",
    "print(\"timm shape\", qkv.shape)\n",
    "# print(\"prisma shape\", cache['blocks.0.attn.hook_qkv'].shape)\n",
    "\n",
    "print(\"prisma q shape\", cache['blocks.0.attn.hook_q'].shape)\n",
    "\n",
    "# assert torch.allclose(qkv, cache['blocks.0.attn.hook_qkv'], atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "assert torch.allclose(q, cache['blocks.0.attn.hook_q'], atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "assert torch.allclose(k, cache['blocks.0.attn.hook_k'], atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "assert torch.allclose(v, cache['blocks.0.attn.hook_v'], atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_q = q * 64 ** -0.5\n",
    "timm_attn_scores = scaled_q @ k.transpose(-2,-1)\n",
    "\n",
    "print(\"timm attn scores\", timm_attn_scores.shape)\n",
    "print(\"prisma attn scores\", cache['blocks.0.attn.hook_attn_scores'].shape)\n",
    "\n",
    "assert torch.allclose(timm_attn_scores, cache['blocks.0.attn.hook_attn_scores'], atol=1e-4), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_attn_pattern = timm_attn_scores.softmax(dim=-1) \n",
    "\n",
    "assert torch.allclose(timm_attn_pattern, cache['blocks.0.attn.hook_pattern'], atol=1e-4), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.blocks[0].attn.proj.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(activations[0].shape)\n",
    "# print(cache['blocks.0.attn.hook_attn_out'].shape)\n",
    "\n",
    "\n",
    "assert torch.allclose(cache['blocks.0.attn.hook_result'], activations[0], atol=1e-3), \"Activations differ more than the allowed tolerance\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#currently only vit_base_patch16_224 supported (config loading issue)\n",
    "TOLERANCE = 1e-5\n",
    "\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "batch_size = 5\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "device = \"cuda\"\n",
    "\n",
    "hooked_model = HookedViT.from_pretrained(model_name)\n",
    "hooked_model.to(device)\n",
    "timm_model = timm.create_model(model_name, pretrained=True)\n",
    "timm_model.to(device)\n",
    "\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(1)\n",
    "    input_image = torch.rand((batch_size, channels, height, width)).to(device)\n",
    "\n",
    "assert torch.allclose(hooked_model(input_image), timm_model(input_image), atol=TOLERANCE), \"Model output diverges!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model(input_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model(input_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prisma_env",
   "language": "python",
   "name": "prisma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
