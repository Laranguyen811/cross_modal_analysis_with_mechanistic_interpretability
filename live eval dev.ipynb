{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a2e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "from vit_prisma.sae.train_sae import VisionSAETrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bf955f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 15869\n",
      "Total training images: 1300000\n",
      "Total wandb updates: 158\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 15 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "Config created\n",
      "Configuration:\n",
      "  model_class_name: HookedViT\n",
      "  model_name: open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "  hook_point_layer: 0\n",
      "  hook_point_head_index: None\n",
      "  context_size: 50\n",
      "  use_cached_activations: False\n",
      "  cached_activations_path: activations/_network_scratch_s_sonia.joseph_datasets_kaggle_datasets/wkcn_TinyCLIP-ViT-40M-32-Text-19M-LAION400M/blocks.9.hook_mlp_out\n",
      "  d_in: 768\n",
      "  activation_fn_str: relu\n",
      "  activation_fn_kwargs: {}\n",
      "  max_grad_norm: 1.0\n",
      "  initialization_method: independent\n",
      "  normalize_activations: null\n",
      "  n_batches_in_buffer: 4\n",
      "  store_batch_size: 2\n",
      "  num_epochs: 1\n",
      "  total_training_images: 1300000\n",
      "  total_training_tokens: 65000000\n",
      "  image_size: 224\n",
      "  device: cuda\n",
      "  seed: 42\n",
      "  dtype: float32\n",
      "  verbose: False\n",
      "  b_dec_init_method: geometric_median\n",
      "  expansion_factor: 16\n",
      "  from_pretrained_path: None\n",
      "  d_sae: 1048\n",
      "  l1_coefficient: 8e-05\n",
      "  lp_norm: 1\n",
      "  lr: 0.0001\n",
      "  lr_scheduler_name: cosineannealingwarmup\n",
      "  lr_warm_up_steps: 500\n",
      "  train_batch_size: 4096\n",
      "  dataset_name: imagenet1k\n",
      "  dataset_path: /workspace/\n",
      "  dataset_train_path: /workspace/ILSVRC/Data/CLS-LOC/train\n",
      "  dataset_val_path: /workspace/ILSVRC/Data/CLS-LOC/val\n",
      "  use_ghost_grads: True\n",
      "  feature_sampling_window: 1000\n",
      "  dead_feature_window: 5000\n",
      "  dead_feature_threshold: 1e-08\n",
      "  log_to_wandb: True\n",
      "  wandb_project: open_clip_vit_b_32_layer_0_None\n",
      "  wandb_entity: None\n",
      "  wandb_log_frequency: 100\n",
      "  n_checkpoints: 10\n",
      "  checkpoint_path: /workspace/checkpoints\n"
     ]
    }
   ],
   "source": [
    "cfg = VisionModelSAERunnerConfig()\n",
    "print(\"Config created\")\n",
    "\n",
    "cfg.lr = 0.0001\n",
    "cfg.l1_coefficient = 0.00008\n",
    "\n",
    "\n",
    "cfg.hook_point_layer = 0\n",
    "cfg.hook_point = \"blocks.0.hook_mlp_out\"\n",
    "\n",
    "cfg.expansion_factor = 16\n",
    "cfg.d_sae = 1048\n",
    "cfg.initialization_method = \"independent\"\n",
    "cfg.normalize_activations = \"null\"\n",
    "\n",
    "cfg.device = \"cuda\"\n",
    "cfg.model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\"\n",
    "cfg.dataset_path = \"/workspace/\"\n",
    "cfg.dataset_train_path = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "cfg.dataset_val_path = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "cfg.checkpoint_path =\"/workspace/checkpoints\"\n",
    "cfg.wandb_project = f\"open_clip_vit_b_32_layer_{cfg.hook_point_layer}_{cfg.hook_point_head_index}\"\n",
    "\n",
    "cfg.train_batch_size = 4096 # tweak store_batch_size instead\n",
    "cfg.n_batches_in_buffer = 4\n",
    "cfg.store_batch_size = 2 #cfg.train_batch_size\n",
    "cfg.d_in = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd539fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k into HookedTransformer\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "here <vit_prisma.dataloaders.imagenet_dataset.ImageNetValidationDataset object at 0x7b42c47dc550>\n",
      "loaded dataloaders\n"
     ]
    }
   ],
   "source": [
    "trainer = VisionSAETrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff64dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2461ae3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.7193, -1.6901, -1.6755,  ..., -1.7339, -1.7339, -1.7339],\n",
      "          [-1.7193, -1.7193, -1.7047,  ..., -1.7339, -1.7339, -1.7339],\n",
      "          [-1.7193, -1.7193, -1.7047,  ..., -1.7339, -1.7339, -1.7339],\n",
      "          ...,\n",
      "          [-0.2156, -0.2156, -0.2594,  ...,  0.1055,  0.1931,  0.4267],\n",
      "          [-0.2010, -0.2156, -0.2302,  ..., -0.0405,  0.2077,  0.3683],\n",
      "          [-0.1718, -0.1718, -0.1572,  ..., -0.0842,  0.1785,  0.2953]],\n",
      "\n",
      "         [[-1.6621, -1.6470, -1.6621,  ..., -1.6921, -1.6921, -1.6921],\n",
      "          [-1.6771, -1.6771, -1.6771,  ..., -1.6921, -1.6921, -1.6921],\n",
      "          [-1.6621, -1.6771, -1.6771,  ..., -1.6921, -1.6921, -1.6921],\n",
      "          ...,\n",
      "          [-1.6470, -1.6470, -1.6771,  ..., -1.4069, -1.3619, -1.1668],\n",
      "          [-1.6320, -1.6470, -1.6621,  ..., -1.4970, -1.3319, -1.2268],\n",
      "          [-1.6170, -1.6170, -1.6170,  ..., -1.4669, -1.3319, -1.2718]],\n",
      "\n",
      "         [[-1.4091, -1.3522, -1.3949,  ..., -1.4233, -1.4233, -1.4233],\n",
      "          [-1.4233, -1.3949, -1.4376,  ..., -1.4233, -1.4233, -1.4233],\n",
      "          [-1.4233, -1.4233, -1.4518,  ..., -1.4233, -1.4233, -1.4233],\n",
      "          ...,\n",
      "          [-1.1247, -1.1247, -1.1532,  ..., -0.9967, -0.9825, -0.8545],\n",
      "          [-1.1105, -1.1247, -1.1389,  ..., -1.0252, -0.9256, -0.8688],\n",
      "          [-1.1105, -1.0963, -1.0963,  ..., -1.0394, -0.9399, -0.9114]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9303,  1.9303,  1.9303,  ...,  0.5289,  0.5873,  0.6895],\n",
      "          [ 1.9303,  1.9303,  1.9303,  ...,  0.7041,  0.6165,  0.4997],\n",
      "          [ 1.9303,  1.9303,  1.9303,  ...,  1.2588,  1.2442,  1.1712],\n",
      "          ...,\n",
      "          [-0.6828, -1.1791, -0.9164,  ..., -1.4711, -1.4857, -1.5003],\n",
      "          [-0.3470, -0.7120, -0.9310,  ..., -1.4857, -1.5003, -1.5003],\n",
      "          [-0.8142, -0.9456, -1.1499,  ..., -1.4857, -1.5003, -1.5003]],\n",
      "\n",
      "         [[ 1.8498,  1.8498,  1.8498,  ..., -0.2963, -0.2213, -0.0862],\n",
      "          [ 1.8498,  1.8498,  1.8498,  ...,  0.1089, -0.0262, -0.1913],\n",
      "          [ 1.8348,  1.8498,  1.8498,  ...,  0.7992,  0.7392,  0.6491],\n",
      "          ...,\n",
      "          [-0.9117, -1.3169, -1.0017,  ..., -1.5570, -1.5720, -1.5870],\n",
      "          [-0.6865, -0.8216, -0.9867,  ..., -1.5720, -1.5870, -1.5870],\n",
      "          [-1.0918, -1.0767, -1.2268,  ..., -1.5720, -1.5870, -1.5870]],\n",
      "\n",
      "         [[ 0.7666,  0.7808,  0.7808,  ..., -1.1674, -1.1532, -1.0252],\n",
      "          [ 0.7808,  0.7808,  0.7808,  ..., -0.5844, -0.6981, -0.8545],\n",
      "          [ 0.7950,  0.7808,  0.7808,  ..., -0.0440, -0.0582, -0.1435],\n",
      "          ...,\n",
      "          [-0.8403, -1.0963, -0.7123,  ..., -1.4233, -1.4518, -1.4518],\n",
      "          [-0.7123, -0.7550, -0.6981,  ..., -1.4376, -1.4518, -1.4518],\n",
      "          [-0.9825, -0.8830, -0.9825,  ..., -1.4376, -1.4518, -1.4518]]]]) tensor([552, 782])\n"
     ]
    }
   ],
   "source": [
    "# for a,b in trainer.activations_store.image_dataloader_eval:\n",
    "#     print(a,b)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4613a271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770a62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.activations_store.get_val_activations_wrapper_one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1dc8975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 1, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c34ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = next(trainer.activations_store.get_batch_tokens_internal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef87c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1cde692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.activations_store.next_batch().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5a242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
