{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a2e827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "from vit_prisma.sae.train_sae import VisionSAETrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bf955f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 15869\n",
      "Total training images: 1300000\n",
      "Total wandb updates: 158\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 15 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "Config created\n"
     ]
    }
   ],
   "source": [
    "cfg = VisionModelSAERunnerConfig()\n",
    "print(\"Config created\")\n",
    "\n",
    "cfg.lr = 0.0001\n",
    "cfg.l1_coefficient = 0.00008\n",
    "\n",
    "\n",
    "cfg.hook_point_layer = 0\n",
    "cfg.hook_point = \"blocks.0.hook_mlp_out\"\n",
    "\n",
    "cfg.expansion_factor = 16\n",
    "cfg.d_sae = 1048\n",
    "cfg.initialization_method = \"independent\"\n",
    "cfg.normalize_activations = \"null\"\n",
    "\n",
    "cfg.device = \"cuda\"\n",
    "cfg.model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\"\n",
    "cfg.dataset_path = \"/workspace/\"\n",
    "cfg.dataset_train_path = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "cfg.dataset_val_path = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "cfg.checkpoint_path =\"/workspace/checkpoints\"\n",
    "cfg.wandb_project = f\"open_clip_vit_b_32_layer_{cfg.hook_point_layer}_{cfg.hook_point_head_index}\"\n",
    "\n",
    "cfg.train_batch_size = 4096 # tweak store_batch_size instead\n",
    "cfg.n_batches_in_buffer = 4\n",
    "cfg.store_batch_size = 2 #cfg.train_batch_size\n",
    "cfg.d_in = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd539fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90k into HookedTransformer\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "here <vit_prisma.dataloaders.imagenet_dataset.ImageNetValidationDataset object at 0x7f397434f3a0>\n",
      "loaded dataloaders\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import vit_prisma\n",
    "importlib.reload(vit_prisma.sae.train_sae)\n",
    "from vit_prisma.sae.train_sae import VisionSAETrainer\n",
    "\n",
    "trainer = VisionSAETrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770a62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts, labels = trainer.activations_store.get_val_activations_one_batch()\n",
    "# contexts.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc6021e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msae\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/ViT-Prisma/src/vit_prisma/sae/train_sae.py:233\u001b[0m, in \u001b[0;36mVisionSAETrainer.val\u001b[0;34m(self, sparse_autoencoder)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, gt_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset:\n\u001b[1;32m    232\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 233\u001b[0m     gt_labels \u001b[38;5;241m=\u001b[39m \u001b[43mgt_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# needs to start with batch_size dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrun_with_cache(images[\u001b[38;5;28;01mNone\u001b[39;00m,:], names_filter\u001b[38;5;241m=\u001b[39msparse_autoencoder\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_point)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer.val(trainer.sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33dc2cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for images, gt_labels in trainer.eval_dataset:\n",
    "    out, cache = trainer.model.run_with_cache(images.to(\"cuda\")[None,:])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f39f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0fcf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea3e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed\n",
      "hook_pos_embed\n",
      "hook_full_embed\n",
      "ln_pre.hook_scale\n",
      "ln_pre.hook_normalized\n",
      "hook_ln_pre\n",
      "blocks.0.hook_resid_pre\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.hook_resid_post\n",
      "blocks.1.hook_resid_pre\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.hook_resid_post\n",
      "blocks.2.hook_resid_pre\n",
      "blocks.2.ln1.hook_scale\n",
      "blocks.2.ln1.hook_normalized\n",
      "blocks.2.attn.hook_q\n",
      "blocks.2.attn.hook_k\n",
      "blocks.2.attn.hook_v\n",
      "blocks.2.attn.hook_attn_scores\n",
      "blocks.2.attn.hook_pattern\n",
      "blocks.2.attn.hook_z\n",
      "blocks.2.hook_attn_out\n",
      "blocks.2.hook_resid_mid\n",
      "blocks.2.ln2.hook_scale\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.2.mlp.hook_pre\n",
      "blocks.2.mlp.hook_post\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.2.hook_resid_post\n",
      "blocks.3.hook_resid_pre\n",
      "blocks.3.ln1.hook_scale\n",
      "blocks.3.ln1.hook_normalized\n",
      "blocks.3.attn.hook_q\n",
      "blocks.3.attn.hook_k\n",
      "blocks.3.attn.hook_v\n",
      "blocks.3.attn.hook_attn_scores\n",
      "blocks.3.attn.hook_pattern\n",
      "blocks.3.attn.hook_z\n",
      "blocks.3.hook_attn_out\n",
      "blocks.3.hook_resid_mid\n",
      "blocks.3.ln2.hook_scale\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.3.mlp.hook_pre\n",
      "blocks.3.mlp.hook_post\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.3.hook_resid_post\n",
      "blocks.4.hook_resid_pre\n",
      "blocks.4.ln1.hook_scale\n",
      "blocks.4.ln1.hook_normalized\n",
      "blocks.4.attn.hook_q\n",
      "blocks.4.attn.hook_k\n",
      "blocks.4.attn.hook_v\n",
      "blocks.4.attn.hook_attn_scores\n",
      "blocks.4.attn.hook_pattern\n",
      "blocks.4.attn.hook_z\n",
      "blocks.4.hook_attn_out\n",
      "blocks.4.hook_resid_mid\n",
      "blocks.4.ln2.hook_scale\n",
      "blocks.4.ln2.hook_normalized\n",
      "blocks.4.mlp.hook_pre\n",
      "blocks.4.mlp.hook_post\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.4.hook_resid_post\n",
      "blocks.5.hook_resid_pre\n",
      "blocks.5.ln1.hook_scale\n",
      "blocks.5.ln1.hook_normalized\n",
      "blocks.5.attn.hook_q\n",
      "blocks.5.attn.hook_k\n",
      "blocks.5.attn.hook_v\n",
      "blocks.5.attn.hook_attn_scores\n",
      "blocks.5.attn.hook_pattern\n",
      "blocks.5.attn.hook_z\n",
      "blocks.5.hook_attn_out\n",
      "blocks.5.hook_resid_mid\n",
      "blocks.5.ln2.hook_scale\n",
      "blocks.5.ln2.hook_normalized\n",
      "blocks.5.mlp.hook_pre\n",
      "blocks.5.mlp.hook_post\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.5.hook_resid_post\n",
      "blocks.6.hook_resid_pre\n",
      "blocks.6.ln1.hook_scale\n",
      "blocks.6.ln1.hook_normalized\n",
      "blocks.6.attn.hook_q\n",
      "blocks.6.attn.hook_k\n",
      "blocks.6.attn.hook_v\n",
      "blocks.6.attn.hook_attn_scores\n",
      "blocks.6.attn.hook_pattern\n",
      "blocks.6.attn.hook_z\n",
      "blocks.6.hook_attn_out\n",
      "blocks.6.hook_resid_mid\n",
      "blocks.6.ln2.hook_scale\n",
      "blocks.6.ln2.hook_normalized\n",
      "blocks.6.mlp.hook_pre\n",
      "blocks.6.mlp.hook_post\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.6.hook_resid_post\n",
      "blocks.7.hook_resid_pre\n",
      "blocks.7.ln1.hook_scale\n",
      "blocks.7.ln1.hook_normalized\n",
      "blocks.7.attn.hook_q\n",
      "blocks.7.attn.hook_k\n",
      "blocks.7.attn.hook_v\n",
      "blocks.7.attn.hook_attn_scores\n",
      "blocks.7.attn.hook_pattern\n",
      "blocks.7.attn.hook_z\n",
      "blocks.7.hook_attn_out\n",
      "blocks.7.hook_resid_mid\n",
      "blocks.7.ln2.hook_scale\n",
      "blocks.7.ln2.hook_normalized\n",
      "blocks.7.mlp.hook_pre\n",
      "blocks.7.mlp.hook_post\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.7.hook_resid_post\n",
      "blocks.8.hook_resid_pre\n",
      "blocks.8.ln1.hook_scale\n",
      "blocks.8.ln1.hook_normalized\n",
      "blocks.8.attn.hook_q\n",
      "blocks.8.attn.hook_k\n",
      "blocks.8.attn.hook_v\n",
      "blocks.8.attn.hook_attn_scores\n",
      "blocks.8.attn.hook_pattern\n",
      "blocks.8.attn.hook_z\n",
      "blocks.8.hook_attn_out\n",
      "blocks.8.hook_resid_mid\n",
      "blocks.8.ln2.hook_scale\n",
      "blocks.8.ln2.hook_normalized\n",
      "blocks.8.mlp.hook_pre\n",
      "blocks.8.mlp.hook_post\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.8.hook_resid_post\n",
      "blocks.9.hook_resid_pre\n",
      "blocks.9.ln1.hook_scale\n",
      "blocks.9.ln1.hook_normalized\n",
      "blocks.9.attn.hook_q\n",
      "blocks.9.attn.hook_k\n",
      "blocks.9.attn.hook_v\n",
      "blocks.9.attn.hook_attn_scores\n",
      "blocks.9.attn.hook_pattern\n",
      "blocks.9.attn.hook_z\n",
      "blocks.9.hook_attn_out\n",
      "blocks.9.hook_resid_mid\n",
      "blocks.9.ln2.hook_scale\n",
      "blocks.9.ln2.hook_normalized\n",
      "blocks.9.mlp.hook_pre\n",
      "blocks.9.mlp.hook_post\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.9.hook_resid_post\n",
      "blocks.10.hook_resid_pre\n",
      "blocks.10.ln1.hook_scale\n",
      "blocks.10.ln1.hook_normalized\n",
      "blocks.10.attn.hook_q\n",
      "blocks.10.attn.hook_k\n",
      "blocks.10.attn.hook_v\n",
      "blocks.10.attn.hook_attn_scores\n",
      "blocks.10.attn.hook_pattern\n",
      "blocks.10.attn.hook_z\n",
      "blocks.10.hook_attn_out\n",
      "blocks.10.hook_resid_mid\n",
      "blocks.10.ln2.hook_scale\n",
      "blocks.10.ln2.hook_normalized\n",
      "blocks.10.mlp.hook_pre\n",
      "blocks.10.mlp.hook_post\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.10.hook_resid_post\n",
      "blocks.11.hook_resid_pre\n",
      "blocks.11.ln1.hook_scale\n",
      "blocks.11.ln1.hook_normalized\n",
      "blocks.11.attn.hook_q\n",
      "blocks.11.attn.hook_k\n",
      "blocks.11.attn.hook_v\n",
      "blocks.11.attn.hook_attn_scores\n",
      "blocks.11.attn.hook_pattern\n",
      "blocks.11.attn.hook_z\n",
      "blocks.11.hook_attn_out\n",
      "blocks.11.hook_resid_mid\n",
      "blocks.11.ln2.hook_scale\n",
      "blocks.11.ln2.hook_normalized\n",
      "blocks.11.mlp.hook_pre\n",
      "blocks.11.mlp.hook_post\n",
      "blocks.11.hook_mlp_out\n",
      "blocks.11.hook_resid_post\n",
      "ln_final.hook_scale\n",
      "ln_final.hook_normalized\n",
      "hook_ln_final\n",
      "hook_post_head_pre_normalize\n"
     ]
    }
   ],
   "source": [
    "for key in cache:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64260dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
