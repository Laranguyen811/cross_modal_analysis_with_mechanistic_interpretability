# %%
import vit_prisma
from vit_prisma.utils.data_utils.imagenet.imagenet_dict import IMAGENET_DICT
from vit_prisma.utils import prisma_utils

#from transformers import CLIPProcessor, CLIPModel

from vit_prisma.utils.data_utils.imagenet.imagenet_utils import imagenet_index_from_word
import numpy as np
import torch as t
from fancy_einsum import einsum
from collections import defaultdict

import plotly.graph_objs as go
import plotly.express as px

import matplotlib.colors as mcolors

from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from IPython.display import display, HTML
from typing import Callable
from jaxtyping import Float, Int
from torch import Tensor
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# %%
# Helper function (ignore)
def plot_image(image):
  plt.figure()
  plt.axis('off')
  plt.imshow(image.permute(1,2,0))

class ConvertTo3Channels:
    def __call__(self, img):
        if img.mode != 'RGB':
            return img.convert('RGB')
        return img

transform = transforms.Compose([
    ConvertTo3Channels(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def plot_logit_boxplot(average_logits, labels):
  hovertexts = np.array([[IMAGENET_DICT[i] for _ in range(25)] for i in range(1000)])

  fig = go.Figure()
  data = []

  # if tensor, turn to numpy
  if isinstance(average_logits, torch.Tensor):
      average_logits = average_logits.detach().cpu().numpy()

  for i in range(average_logits.shape[1]):  # For each layer
      layer_logits = average_logits[:, i]
      hovertext = hovertexts[:, i]
      box = fig.add_trace(go.Box(
          y=layer_logits,
          name=f'{layer_labels[i]}',
          text=hovertext,
          hoverinfo='y+text',
          boxpoints='suspectedoutliers'
      ))
      data.append(box)


  means = np.mean(average_logits, axis=0)
  fig.add_trace(go.Scatter(
      x = layer_labels,
      y=means,
      mode='markers',
      name='Mean',
      # line=dict(color='gray'),
      marker=dict(size=4, color='red'),
  ))


  fig.update_layout(
      title='Raw Logit Values Per Layer (each dot is 1 ImageNet Class)',
      xaxis=dict(title='Layer'),
      yaxis=dict(title='Logit Values'),
      showlegend=False
  )

  fig.show()


# %%
def plot_patched_component(patched_head, title=''):
  """
  Use for plotting Activation Patching.
  """

  fig = go.Figure(data=go.Heatmap(
      z=patched_head.detach().numpy(),
      colorscale='RdBu',  # You can choose any colorscale
      colorbar=dict(title='Value'),  # Customize the color bar
      hoverongaps=False
  ))
  fig.update_layout(
      title=title,
      xaxis_title='Attention Head',
      yaxis_title='Patch Number',
  )

  return fig

def imshow(tensor, **kwargs):
    """
    Use for Activation Patching.
    """
    px.imshow(
          prisma_utils.to_numpy(tensor),
          color_continuous_midpoint=0.0,
          color_continuous_scale="RdBu",
          **kwargs,
      ).show()

# %%
# We'll use a text-image transformer. Loading the model
from transformers import CLIPProcessor, CLIPModel
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# %%
transform = transforms.Compose([
   transforms.Resize((224, 224)),
    transforms.ToTensor(),
])
# Download CIFAR-10 dataset
cifar_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
cifar_loader = DataLoader(cifar_dataset, batch_size=8, shuffle=True)
num_batches = 30  # Set how many batches you want to process
cifar_labels = cifar_dataset.classes  # Get class names once

for batch_idx, (images, labels) in enumerate(cifar_loader):
    pil_images = [transforms.ToPILImage()(img) for img in images]
    text_labels = [f"a photo of a {cifar_labels[label]}" for label in labels]

    if batch_idx + 1 >= num_batches:
        break
# CIFAR-10 class names
#cifar_labels = cifar_dataset.classes

# Get a batch of images and labels
#images, labels = next(iter(cifar_loader))

# Convert tensor images to PIL images for visualization or further processing
#pil_images = [transforms.ToPILImage()(img) for img in images]
#text_labels = [f"a photo of a {cifar_labels[label]}" for label in labels]


# %%
# Loading the image
device = "cuda" if t.cuda.is_available() else "cpu"
model = model.to(device)

# Use the batch of images and labels generated earlier
# images: tensor of shape [batch_size, 3, 224, 224]
# text_labels: list of strings like ["a photo of a cat", "a photo of a dog", ...]

# Prepare text and image inputs for the whole batch
text_inputs = processor(text=text_labels, return_tensors="pt", padding=True).to(device)
image_inputs = processor(images=pil_images, return_tensors="pt")
image_inputs["pixel_values"] = image_inputs["pixel_values"].to(device)

with t.no_grad():
    outputs = model(**image_inputs, **text_inputs, output_hidden_states=True)
    image_logits = outputs.logits_per_image
    text_logits = outputs.logits_per_text
    probs = outputs.logits_per_image.softmax(dim=1).cpu().numpy()

print("Image Logits:", image_logits)
print("Text Logits:", text_logits)
print("Label probs:", probs)
# %%
#from vit_prisma.visualization.visualize_image import display_grid_on_image

#display_grid_on_image(images, patch_size=32)

# %%
#from vit_prisma.utils.data_utils.imagenet.imagenet_utils import imagenet_index_from_word
#from vit_prisma.utils.data_utils.imagenet.imagenet_dict import IMAGENET_DICT
#from vit_prisma.utils.prisma_utils import test_prompt




# %%
# Find cosine similarities
cosine_similarities = t.cosine_similarity(image_logits, text_logits, dim=0)
print(f"cosine similarities: {cosine_similarities}")
# Make text_labels unique by appending the image index
unique_text_labels = [f"{label} ({i})" for i, label in enumerate(text_labels)]
fig = px.bar(
    x = unique_text_labels,
    y = cosine_similarities.detach().cpu().numpy(),
    title = "Cosine Similarities between Image and Text Logits",
    labels = {'x': 'Text Labels', 'y': 'Cosine Similarities'},
)
fig.show()
# %%
# Accumulated residual
for i,label_indx in enumerate(labels):
   class_name = cifar_labels[label_indx]
   print(f" Image {i}: class name = {class_name}, class index: {label_indx}")

# %%
# Finding the residual directions
# For Images (ViT/CLIP): the residual directions for a class in the column of the classification head weight matrix for that class. 
# For texts (CLIP): the residual direction for a label is the embedding for that label 
image_residual_directions = model.visual_projection.weight[label_indx]
print(f"Visual residual direction: {image_residual_directions.shape}")
with t.no_grad():
   text_residual_directions = model.get_text_features(**text_inputs)
print(f"Text residual directions: {text_residual_directions.shape}") 

# %%
# Access the hidden states

vision_hidden_states = outputs.vision_model_output.hidden_states
text_hidden_states = outputs.text_model_output.hidden_states

print("\n--- Vision Model Hidden States ---")
print(f"Type: {type(vision_hidden_states)}")  # This will be a tuple
print(f"Number of layers (including embeddings): {len(vision_hidden_states)}")

# Get the hidden states from the last layer of the vision model
last_layer_vision_hidden_states = vision_hidden_states[-1]
print(f"Shape of the last layer hidden states: {last_layer_vision_hidden_states.shape}")

print("\n--- Text Model Hidden States ---")
print(f"Type: {type(text_hidden_states)}")  # This will be a tuple
print(f"Number of layers (including embeddings): {len(text_hidden_states)}")

# Get the hidden state from the last layer of the text model
last_layer_text_hidden_states = text_hidden_states[-1]
print(f"Shape of the last layer hidden states: {last_layer_text_hidden_states.shape}")

# Get the classification token at the beginning
cls_vision_residual_stream = last_layer_vision_hidden_states[:, 0, :]
cls_text_residual_stream = last_layer_text_hidden_states[:, 0, :]
print(f"Shape of the CLS token in vision model: {cls_vision_residual_stream.shape}")
print(f"Shape of the CLS token in text model: {cls_text_residual_stream.shape}")

# Get CLIP's LayerNorm
vision_ln = model.vision_model.post_layernorm
text_ln = model.text_model.final_layer_norm

# Apply the LayerNorm to the residual streams
scaled_cls_vision = vision_ln(cls_vision_residual_stream)
scaled_cls_text = text_ln(cls_text_residual_stream)

# Print the shapes of the scaled classification tokens
print(f"Scaled CLS Vision shape: {scaled_cls_vision.shape}")
print(f"Scaled CLS Text shape: {scaled_cls_text.shape}")

# Compute logits for each image-text pair in the batch
# image_residual_directions: [batch, d_model] or [d_model]
# text_residual_directions: [batch, d_model] or [d_model]
# If you want to compute the logit for each image-text pair, use einsum over the batch
vision_logit = einsum('b d,d->b', scaled_cls_vision, image_residual_directions)
text_logit = einsum('b d,b d->b', scaled_cls_text, text_residual_directions)

print(f"Vision logit shape: {vision_logit.shape}")
print(f"Text logit shape: {text_logit.shape}")

# Add the bias for each class to the vision and text logits if present
if getattr(model.visual_projection, 'bias', None) is not None:
    vision_logit += model.visual_projection.bias[labels]
else:
    print("No bias in the visual projection layer.")

if getattr(model.text_projection, 'bias', None) is not None:
    text_logit += model.text_projection.bias[labels]
else:
    print("No bias in the text projection layer.")

# Print the logits for the batch
for i in range(len(vision_logit)):
    print(f"Image {i}: Vision logit: {round(vision_logit[i].item(),3)}, "
          f"Vision logit from model: {round(image_logits[i, i].item(),3)}, "
          f"Text logit: {round(text_logit[i].item(),3)}, "
          f"Text logit from model: {round(text_logits[i, i].item(),3)}")
    
import plotly.graph_objs as go

# Prepare unique labels for each image
unique_labels = [f"{label} ({i})" for i, label in enumerate(text_labels)]

# Convert logits to numpy for plotting
vision_logit_np = vision_logit.detach().cpu().numpy()
text_logit_np = text_logit.detach().cpu().numpy()

# Create a grouped bar chart for direct logit attributions
fig = go.Figure(data=[
    go.Bar(name='Vision Logit', x=unique_labels, y=vision_logit_np),
    go.Bar(name='Text Logit', x=unique_labels, y=text_logit_np)
])

fig.update_layout(
    barmode='group',
    title='Direct Logit Attributions per Image',
    xaxis_title='Text Labels (Image Index)',
    yaxis_title='Logit Value'
)

fig.show()
# %%
# Get the embeddings
image_embeds = outputs.image_embeds
text_embeds = outputs.text_embeds
print(f"Image Embeddings: {image_embeds.shape}")
print(f"Text Embeddings: {text_embeds.shape}")

# %%
# Cosine similarities for embeddings
cosine_similarities = t.cosine_similarity(image_embeds, text_embeds, dim = 1)
print(f"Cosine similarities in the embedding space: {cosine_similarities}")

# %%
# Convert the tensor to a numpy array for plotting

import plotly.express as px
cosine_similarities_np = cosine_similarities.detach().cpu().numpy()
import plotly.io as pio
pio.renderers.default = "browser"  # or "vscode" or "notebook_connected"

# Make text_labels unique by appending the image index
unique_text_labels = [f"{label} ({i})" for i, label in enumerate(text_labels)]

fig = px.bar(
    x = unique_text_labels,
    y = cosine_similarities_np,
    title = "Cosine Similarities between Image and Text Embeddings",
    labels = {'x': 'Text Labels', 'y': 'Cosine Similarities'},
)
fig.show()


# similarity_range = cosine_similarities.max() - cosine_similarities.min()
similarity_range = cosine_similarities.max() - cosine_similarities.min()
if similarity_range < 0.1:
  print("Poor alignment")


# %%
# Mean ablation
def mean_ablation(
      z:Float[Tensor,'batch seq d_model'],
      y:Float[Tensor,'batch d_model'],
      hook: prisma_utils.Hook,
      head_index_to_ablate:int,
)-> None:
   """
   Ablate the mean of the head's output.
   """
   # Get the text head's output
   text_head_output = hook.get_head_output(z, head_index_to_ablate)
   # Compute the mean of the head's output
   text_mean = text_head_output.mean(dim=0, keepdim=True)
   # Subtract the mean from the head's output
   z -= text_mean
   # Return the ablated output
   return z
   # Get the image head's output
   image_head_output = hook.get_head_output(y, head_index_to_ablate)
    # Compute the mean of the image head's output
   image_mean = image_head_output.mean(dim=0, keepdim=True)
    # Subtract the mean from the image head's output
   y -= image_mean
   return y

