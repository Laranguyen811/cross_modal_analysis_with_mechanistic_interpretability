{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis on the Semantic Space of TinyCLIP\n",
    "\n",
    "We want to see the semantic space of TinyCLIP using ablations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vit_prisma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model\n",
    "\n",
    "We're just using the visual encoder of TinyCLIP. The visual encoder is a transformer with 10 layers, 12 attention heads, and 256 hidden representation dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\")\n",
    "processor = CLIPProcessor.from_pretrained(\"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionTransformer(\n",
       "  (embeddings): CLIPVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 512, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (position_embedding): Embedding(50, 512)\n",
       "  )\n",
       "  (pre_layrnorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder): CLIPEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.class_embedding\n",
      "embeddings.patch_embedding.weight\n",
      "embeddings.position_embedding.weight\n",
      "pre_layrnorm.weight\n",
      "pre_layrnorm.bias\n",
      "encoder.layers.0.self_attn.k_proj.weight\n",
      "encoder.layers.0.self_attn.k_proj.bias\n",
      "encoder.layers.0.self_attn.v_proj.weight\n",
      "encoder.layers.0.self_attn.v_proj.bias\n",
      "encoder.layers.0.self_attn.q_proj.weight\n",
      "encoder.layers.0.self_attn.q_proj.bias\n",
      "encoder.layers.0.self_attn.out_proj.weight\n",
      "encoder.layers.0.self_attn.out_proj.bias\n",
      "encoder.layers.0.layer_norm1.weight\n",
      "encoder.layers.0.layer_norm1.bias\n",
      "encoder.layers.0.mlp.fc1.weight\n",
      "encoder.layers.0.mlp.fc1.bias\n",
      "encoder.layers.0.mlp.fc2.weight\n",
      "encoder.layers.0.mlp.fc2.bias\n",
      "encoder.layers.0.layer_norm2.weight\n",
      "encoder.layers.0.layer_norm2.bias\n",
      "encoder.layers.1.self_attn.k_proj.weight\n",
      "encoder.layers.1.self_attn.k_proj.bias\n",
      "encoder.layers.1.self_attn.v_proj.weight\n",
      "encoder.layers.1.self_attn.v_proj.bias\n",
      "encoder.layers.1.self_attn.q_proj.weight\n",
      "encoder.layers.1.self_attn.q_proj.bias\n",
      "encoder.layers.1.self_attn.out_proj.weight\n",
      "encoder.layers.1.self_attn.out_proj.bias\n",
      "encoder.layers.1.layer_norm1.weight\n",
      "encoder.layers.1.layer_norm1.bias\n",
      "encoder.layers.1.mlp.fc1.weight\n",
      "encoder.layers.1.mlp.fc1.bias\n",
      "encoder.layers.1.mlp.fc2.weight\n",
      "encoder.layers.1.mlp.fc2.bias\n",
      "encoder.layers.1.layer_norm2.weight\n",
      "encoder.layers.1.layer_norm2.bias\n",
      "encoder.layers.2.self_attn.k_proj.weight\n",
      "encoder.layers.2.self_attn.k_proj.bias\n",
      "encoder.layers.2.self_attn.v_proj.weight\n",
      "encoder.layers.2.self_attn.v_proj.bias\n",
      "encoder.layers.2.self_attn.q_proj.weight\n",
      "encoder.layers.2.self_attn.q_proj.bias\n",
      "encoder.layers.2.self_attn.out_proj.weight\n",
      "encoder.layers.2.self_attn.out_proj.bias\n",
      "encoder.layers.2.layer_norm1.weight\n",
      "encoder.layers.2.layer_norm1.bias\n",
      "encoder.layers.2.mlp.fc1.weight\n",
      "encoder.layers.2.mlp.fc1.bias\n",
      "encoder.layers.2.mlp.fc2.weight\n",
      "encoder.layers.2.mlp.fc2.bias\n",
      "encoder.layers.2.layer_norm2.weight\n",
      "encoder.layers.2.layer_norm2.bias\n",
      "encoder.layers.3.self_attn.k_proj.weight\n",
      "encoder.layers.3.self_attn.k_proj.bias\n",
      "encoder.layers.3.self_attn.v_proj.weight\n",
      "encoder.layers.3.self_attn.v_proj.bias\n",
      "encoder.layers.3.self_attn.q_proj.weight\n",
      "encoder.layers.3.self_attn.q_proj.bias\n",
      "encoder.layers.3.self_attn.out_proj.weight\n",
      "encoder.layers.3.self_attn.out_proj.bias\n",
      "encoder.layers.3.layer_norm1.weight\n",
      "encoder.layers.3.layer_norm1.bias\n",
      "encoder.layers.3.mlp.fc1.weight\n",
      "encoder.layers.3.mlp.fc1.bias\n",
      "encoder.layers.3.mlp.fc2.weight\n",
      "encoder.layers.3.mlp.fc2.bias\n",
      "encoder.layers.3.layer_norm2.weight\n",
      "encoder.layers.3.layer_norm2.bias\n",
      "encoder.layers.4.self_attn.k_proj.weight\n",
      "encoder.layers.4.self_attn.k_proj.bias\n",
      "encoder.layers.4.self_attn.v_proj.weight\n",
      "encoder.layers.4.self_attn.v_proj.bias\n",
      "encoder.layers.4.self_attn.q_proj.weight\n",
      "encoder.layers.4.self_attn.q_proj.bias\n",
      "encoder.layers.4.self_attn.out_proj.weight\n",
      "encoder.layers.4.self_attn.out_proj.bias\n",
      "encoder.layers.4.layer_norm1.weight\n",
      "encoder.layers.4.layer_norm1.bias\n",
      "encoder.layers.4.mlp.fc1.weight\n",
      "encoder.layers.4.mlp.fc1.bias\n",
      "encoder.layers.4.mlp.fc2.weight\n",
      "encoder.layers.4.mlp.fc2.bias\n",
      "encoder.layers.4.layer_norm2.weight\n",
      "encoder.layers.4.layer_norm2.bias\n",
      "encoder.layers.5.self_attn.k_proj.weight\n",
      "encoder.layers.5.self_attn.k_proj.bias\n",
      "encoder.layers.5.self_attn.v_proj.weight\n",
      "encoder.layers.5.self_attn.v_proj.bias\n",
      "encoder.layers.5.self_attn.q_proj.weight\n",
      "encoder.layers.5.self_attn.q_proj.bias\n",
      "encoder.layers.5.self_attn.out_proj.weight\n",
      "encoder.layers.5.self_attn.out_proj.bias\n",
      "encoder.layers.5.layer_norm1.weight\n",
      "encoder.layers.5.layer_norm1.bias\n",
      "encoder.layers.5.mlp.fc1.weight\n",
      "encoder.layers.5.mlp.fc1.bias\n",
      "encoder.layers.5.mlp.fc2.weight\n",
      "encoder.layers.5.mlp.fc2.bias\n",
      "encoder.layers.5.layer_norm2.weight\n",
      "encoder.layers.5.layer_norm2.bias\n",
      "encoder.layers.6.self_attn.k_proj.weight\n",
      "encoder.layers.6.self_attn.k_proj.bias\n",
      "encoder.layers.6.self_attn.v_proj.weight\n",
      "encoder.layers.6.self_attn.v_proj.bias\n",
      "encoder.layers.6.self_attn.q_proj.weight\n",
      "encoder.layers.6.self_attn.q_proj.bias\n",
      "encoder.layers.6.self_attn.out_proj.weight\n",
      "encoder.layers.6.self_attn.out_proj.bias\n",
      "encoder.layers.6.layer_norm1.weight\n",
      "encoder.layers.6.layer_norm1.bias\n",
      "encoder.layers.6.mlp.fc1.weight\n",
      "encoder.layers.6.mlp.fc1.bias\n",
      "encoder.layers.6.mlp.fc2.weight\n",
      "encoder.layers.6.mlp.fc2.bias\n",
      "encoder.layers.6.layer_norm2.weight\n",
      "encoder.layers.6.layer_norm2.bias\n",
      "encoder.layers.7.self_attn.k_proj.weight\n",
      "encoder.layers.7.self_attn.k_proj.bias\n",
      "encoder.layers.7.self_attn.v_proj.weight\n",
      "encoder.layers.7.self_attn.v_proj.bias\n",
      "encoder.layers.7.self_attn.q_proj.weight\n",
      "encoder.layers.7.self_attn.q_proj.bias\n",
      "encoder.layers.7.self_attn.out_proj.weight\n",
      "encoder.layers.7.self_attn.out_proj.bias\n",
      "encoder.layers.7.layer_norm1.weight\n",
      "encoder.layers.7.layer_norm1.bias\n",
      "encoder.layers.7.mlp.fc1.weight\n",
      "encoder.layers.7.mlp.fc1.bias\n",
      "encoder.layers.7.mlp.fc2.weight\n",
      "encoder.layers.7.mlp.fc2.bias\n",
      "encoder.layers.7.layer_norm2.weight\n",
      "encoder.layers.7.layer_norm2.bias\n",
      "encoder.layers.8.self_attn.k_proj.weight\n",
      "encoder.layers.8.self_attn.k_proj.bias\n",
      "encoder.layers.8.self_attn.v_proj.weight\n",
      "encoder.layers.8.self_attn.v_proj.bias\n",
      "encoder.layers.8.self_attn.q_proj.weight\n",
      "encoder.layers.8.self_attn.q_proj.bias\n",
      "encoder.layers.8.self_attn.out_proj.weight\n",
      "encoder.layers.8.self_attn.out_proj.bias\n",
      "encoder.layers.8.layer_norm1.weight\n",
      "encoder.layers.8.layer_norm1.bias\n",
      "encoder.layers.8.mlp.fc1.weight\n",
      "encoder.layers.8.mlp.fc1.bias\n",
      "encoder.layers.8.mlp.fc2.weight\n",
      "encoder.layers.8.mlp.fc2.bias\n",
      "encoder.layers.8.layer_norm2.weight\n",
      "encoder.layers.8.layer_norm2.bias\n",
      "encoder.layers.9.self_attn.k_proj.weight\n",
      "encoder.layers.9.self_attn.k_proj.bias\n",
      "encoder.layers.9.self_attn.v_proj.weight\n",
      "encoder.layers.9.self_attn.v_proj.bias\n",
      "encoder.layers.9.self_attn.q_proj.weight\n",
      "encoder.layers.9.self_attn.q_proj.bias\n",
      "encoder.layers.9.self_attn.out_proj.weight\n",
      "encoder.layers.9.self_attn.out_proj.bias\n",
      "encoder.layers.9.layer_norm1.weight\n",
      "encoder.layers.9.layer_norm1.bias\n",
      "encoder.layers.9.mlp.fc1.weight\n",
      "encoder.layers.9.mlp.fc1.bias\n",
      "encoder.layers.9.mlp.fc2.weight\n",
      "encoder.layers.9.mlp.fc2.bias\n",
      "encoder.layers.9.layer_norm2.weight\n",
      "encoder.layers.9.layer_norm2.bias\n",
      "encoder.layers.10.self_attn.k_proj.weight\n",
      "encoder.layers.10.self_attn.k_proj.bias\n",
      "encoder.layers.10.self_attn.v_proj.weight\n",
      "encoder.layers.10.self_attn.v_proj.bias\n",
      "encoder.layers.10.self_attn.q_proj.weight\n",
      "encoder.layers.10.self_attn.q_proj.bias\n",
      "encoder.layers.10.self_attn.out_proj.weight\n",
      "encoder.layers.10.self_attn.out_proj.bias\n",
      "encoder.layers.10.layer_norm1.weight\n",
      "encoder.layers.10.layer_norm1.bias\n",
      "encoder.layers.10.mlp.fc1.weight\n",
      "encoder.layers.10.mlp.fc1.bias\n",
      "encoder.layers.10.mlp.fc2.weight\n",
      "encoder.layers.10.mlp.fc2.bias\n",
      "encoder.layers.10.layer_norm2.weight\n",
      "encoder.layers.10.layer_norm2.bias\n",
      "encoder.layers.11.self_attn.k_proj.weight\n",
      "encoder.layers.11.self_attn.k_proj.bias\n",
      "encoder.layers.11.self_attn.v_proj.weight\n",
      "encoder.layers.11.self_attn.v_proj.bias\n",
      "encoder.layers.11.self_attn.q_proj.weight\n",
      "encoder.layers.11.self_attn.q_proj.bias\n",
      "encoder.layers.11.self_attn.out_proj.weight\n",
      "encoder.layers.11.self_attn.out_proj.bias\n",
      "encoder.layers.11.layer_norm1.weight\n",
      "encoder.layers.11.layer_norm1.bias\n",
      "encoder.layers.11.mlp.fc1.weight\n",
      "encoder.layers.11.mlp.fc1.bias\n",
      "encoder.layers.11.mlp.fc2.weight\n",
      "encoder.layers.11.mlp.fc2.bias\n",
      "encoder.layers.11.layer_norm2.weight\n",
      "encoder.layers.11.layer_norm2.bias\n",
      "post_layernorm.weight\n",
      "post_layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "for k in model.vision_model.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.cls_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model = ViTForImageClassification.from_pretrained(\n",
    "                    model_name,\n",
    "            )\n",
    "\n",
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token torch.Size([1, 1, 768])\n",
      "vit.embeddings.position_embeddings torch.Size([1, 197, 768])\n",
      "vit.embeddings.patch_embeddings.projection.weight torch.Size([768, 3, 16, 16])\n",
      "vit.embeddings.patch_embeddings.projection.bias torch.Size([768])\n",
      "vit.encoder.layer.0.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.0.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.0.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.1.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.1.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.1.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.2.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.2.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.2.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.3.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.3.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.3.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.4.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.4.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.4.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.5.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.5.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.5.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.6.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.6.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.6.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.7.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.7.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.7.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.8.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.8.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.8.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.9.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.9.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.9.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.10.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.10.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.10.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_after.bias torch.Size([768])\n",
      "vit.encoder.layer.11.attention.attention.query.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.attention.query.bias torch.Size([768])\n",
      "vit.encoder.layer.11.attention.attention.key.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.attention.key.bias torch.Size([768])\n",
      "vit.encoder.layer.11.attention.attention.value.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.attention.value.bias torch.Size([768])\n",
      "vit.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "vit.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "vit.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_before.weight torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_before.bias torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_after.weight torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_after.bias torch.Size([768])\n",
      "vit.layernorm.weight torch.Size([768])\n",
      "vit.layernorm.bias torch.Size([768])\n",
      "classifier.weight torch.Size([2, 768])\n",
      "classifier.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, i in hf_model.named_parameters():\n",
    "    print(name, i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_config CLIPVisionConfig {\n",
      "  \"architecture\": \"vit_clip_vision_encoder\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_classes\": \"n/a\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 32,\n",
      "  \"projection_dim\": 512,\n",
      "  \"transformers_version\": \"4.37.2\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.cls_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading weights from the architecture is not currently supported: vit_clip_vision_encoder, generated from model name . Feel free to open an issue on GitHub to request this feature.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/ViT-Planetarium/src/vit_prisma/prisma/loading_from_pretrained.py:134\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, is_timm, cfg, hf_model, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# state_dict = None # Conversion of state dict to HookedTransformer format       \u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_timm_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/ViT-Planetarium/src/vit_prisma/prisma/loading_from_pretrained.py:38\u001b[0m, in \u001b[0;36mconvert_timm_weights\u001b[0;34m(old_state_dict, cfg)\u001b[0m\n\u001b[1;32m     37\u001b[0m new_state_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 38\u001b[0m new_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_token\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mold_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcls_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m new_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_embed.W_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m old_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cls_token'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvit_prisma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_vit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedViT\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtinyclip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m prisma_model \u001b[38;5;241m=\u001b[39m \u001b[43mHookedViT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_timm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ViT-Planetarium/src/vit_prisma/models/base_vit.py:645\u001b[0m, in \u001b[0;36mHookedViT.from_pretrained\u001b[0;34m(cls, model_name, is_timm, is_clip, fold_ln, center_writing_weights, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16 models may not work on CPU. Consider using a GPU or bfloat16.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m     )\n\u001b[1;32m    639\u001b[0m cfg \u001b[38;5;241m=\u001b[39m convert_pretrained_model_config(\n\u001b[1;32m    640\u001b[0m     model_name,\n\u001b[1;32m    641\u001b[0m     is_timm\u001b[38;5;241m=\u001b[39mis_timm,\n\u001b[1;32m    642\u001b[0m     is_clip\u001b[38;5;241m=\u001b[39mis_clip,\n\u001b[1;32m    643\u001b[0m )\n\u001b[0;32m--> 645\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_pretrained_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_timm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(cfg, move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    651\u001b[0m model\u001b[38;5;241m.\u001b[39mload_and_process_state_dict(\n\u001b[1;32m    652\u001b[0m     state_dict,\n\u001b[1;32m    653\u001b[0m     fold_ln\u001b[38;5;241m=\u001b[39mfold_ln,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m     refactor_factored_attn_matrices\u001b[38;5;241m=\u001b[39mrefactor_factored_attn_matrices,\n\u001b[1;32m    657\u001b[0m )\n",
      "File \u001b[0;32m~/ViT-Planetarium/src/vit_prisma/prisma/loading_from_pretrained.py:139\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, is_timm, cfg, hf_model, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_dict\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights from the architecture is not currently supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39moriginal_architecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, generated from model name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Feel free to open an issue on GitHub to request this feature.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Loading weights from the architecture is not currently supported: vit_clip_vision_encoder, generated from model name . Feel free to open an issue on GitHub to request this feature."
     ]
    }
   ],
   "source": [
    "from vit_prisma.configs import HookedViTConfig\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "'tinyclip'\n",
    "\n",
    "prisma_model = HookedViT.from_pretrained(\"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\", is_timm=False, is_clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\")\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CIFAR-100\n",
    "\n",
    "The labels of CIFAR-100 have two levels of granularity: coarse-grained and fine-grained labels. \n",
    "\n",
    "We want to see the relationship between the coarse-grained and fine-grained labels inside the net. For example, perhaps the coarse-grained labels tend to be identified around Layer 5, while the fine-grained labels tend to be identified around Label 6. Perhaps there is a semantic hierarchy reflected in a TinyCLIP circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prisma_env",
   "language": "python",
   "name": "prisma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
