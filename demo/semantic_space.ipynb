{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis on the Semantic Space of a Vision Transformer\n",
    "\n",
    "We want to see the semantic space of TinyCLIP using ablations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vit_prisma \n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model\n",
    "\n",
    "We're just using the visual encoder of TinyCLIP. The visual encoder is a transformer with 10 layers, 12 attention heads, and 256 hidden representation dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 12, 'd_model': 768, 'd_head': 64, 'model_name': 'timm/vit_base_patch32_224.augreg_in21k_ft_in1k', 'n_heads': 12, 'd_mlp': 3072, 'activation_name': 'gelu', 'eps': 1e-06, 'original_architecture': 'vit_base_patch32_224', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 32, 'image_size': 224, 'n_classes': 1000, 'n_params': 88224232, 'return_type': 'class_logits'}\n",
      "Loaded pretrained model vit_base_patch32_224 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "import torch\n",
    "\n",
    "TOLERANCE = 1e-5\n",
    "\n",
    "model_name = \"vit_base_patch32_224\"\n",
    "batch_size = 5\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "device = \"cpu\"\n",
    "\n",
    "hooked_model = HookedViT.from_pretrained(model_name)\n",
    "hooked_model.to(device)\n",
    "timm_model = timm.create_model(model_name, pretrained=True)\n",
    "timm_model.to(device)\n",
    "\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(1)\n",
    "    input_image = torch.rand((batch_size, channels, height, width)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(hooked_model(input_image), timm_model(input_image), atol=TOLERANCE), \"Model output diverges!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the model so it's performing well on CIFAR-100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CIFAR-100\n",
    "\n",
    "The labels of CIFAR-100 have two levels of granularity: coarse-grained and fine-grained labels. \n",
    "\n",
    "We want to see the relationship between the coarse-grained and fine-grained labels inside the net. For example, perhaps the coarse-grained labels tend to be identified around Layer 5, while the fine-grained labels tend to be identified around Label 6. Perhaps there is a semantic hierarchy reflected in a TinyCLIP circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "\n",
    "# Set the REQUESTS_CA_BUNDLE environment variable to the certifi CA bundle path\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['SSL_CERT_DIR'] = '/etc/ssl/certs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images to [-1, 1]\n",
    "    # Resize to 224 x 224\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "\n",
    "testset = datasets.CIFAR100(root='/home/mila/s/sonia.joseph/ViT-Planetarium/data/cifar100', train=False, download=True, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "superclasses = {\n",
    "    \"aquatic mammals\": [\"beaver\", \"dolphin\", \"otter\", \"seal\", \"whale\"],\n",
    "    \"fish\": [\"aquarium fish\", \"flatfish\", \"ray\", \"shark\", \"trout\"],\n",
    "    \"flowers\": [\"orchids\", \"poppies\", \"roses\", \"sunflowers\", \"tulips\"],\n",
    "    \"food containers\": [\"bottles\", \"bowls\", \"cans\", \"cups\", \"plates\"],\n",
    "    \"fruit and vegetables\": [\"apples\", \"mushrooms\", \"oranges\", \"pears\", \"sweet peppers\"],\n",
    "    \"household electrical devices\": [\"clock\", \"computer keyboard\", \"lamp\", \"telephone\", \"television\"],\n",
    "    \"household furniture\": [\"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\"],\n",
    "    \"insects\": [\"bee\", \"beetle\", \"butterfly\", \"caterpillar\", \"cockroach\"],\n",
    "    \"large carnivores\": [\"bear\", \"leopard\", \"lion\", \"tiger\", \"wolf\"],\n",
    "    \"large man-made outdoor things\": [\"bridge\", \"castle\", \"house\", \"road\", \"skyscraper\"],\n",
    "    \"large natural outdoor scenes\": [\"cloud\", \"forest\", \"mountain\", \"plain\", \"sea\"],\n",
    "    \"large omnivores and herbivores\": [\"camel\", \"cattle\", \"chimpanzee\", \"elephant\", \"kangaroo\"],\n",
    "    \"medium-sized mammals\": [\"fox\", \"porcupine\", \"possum\", \"raccoon\", \"skunk\"],\n",
    "    \"non-insect invertebrates\": [\"crab\", \"lobster\", \"snail\", \"spider\", \"worm\"],\n",
    "    \"people\": [\"baby\", \"boy\", \"girl\", \"man\", \"woman\"],\n",
    "    \"reptiles\": [\"crocodile\", \"dinosaur\", \"lizard\", \"snake\", \"turtle\"],\n",
    "    \"small mammals\": [\"hamster\", \"mouse\", \"rabbit\", \"shrew\", \"squirrel\"],\n",
    "    \"trees\": [\"maple\", \"oak\", \"palm\", \"pine\", \"willow\"],\n",
    "    \"vehicles 1\": [\"bicycle\", \"bus\", \"motorcycle\", \"pickup truck\", \"train\"],\n",
    "    \"vehicles 2\": [\"lawn-mower\", \"rocket\", \"streetcar\", \"tank\", \"tractor\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superclass(class_name, superclasses):\n",
    "    \"\"\"\n",
    "    Return the superclass of a given class name from CIFAR-100.\n",
    "\n",
    "    Parameters:\n",
    "    - class_name: The name of the class for which to find the superclass.\n",
    "    - superclasses: A dictionary where keys are superclasses and values are lists of classes.\n",
    "\n",
    "    Returns:\n",
    "    - The name of the superclass if found, otherwise \"Class not found\".\n",
    "    \"\"\"\n",
    "    for superclass, classes in superclasses.items():\n",
    "        if class_name in classes:\n",
    "            return superclass\n",
    "    return \"Class not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 12, 'd_model': 768, 'd_head': 64, 'model_name': 'timm/vit_base_patch32_224.augreg_in21k_ft_in1k', 'n_heads': 12, 'd_mlp': 3072, 'activation_name': 'gelu', 'eps': 1e-06, 'original_architecture': 'vit_base_patch32_224', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 32, 'image_size': 224, 'n_classes': 1000, 'n_params': 88224232, 'return_type': 'class_logits'}\n",
      "Loaded pretrained model vit_base_patch32_224 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hooked_model = HookedViT.from_pretrained(model_name,\n",
    "                                        center_unembed=True,\n",
    "                                        center_writing_weights=True,\n",
    "                                        fold_ln=True,\n",
    "                                        refactor_factored_attn_matrices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Return a table that is:\n",
    "# Rank: Logit: Prob: Class:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prisma_env",
   "language": "python",
   "name": "prisma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
